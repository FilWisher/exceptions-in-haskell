\chapter{Background}

This chapter explores what \emph{formal systems} are and what they are useful for. It looks at a number of related formal systems and their relation to computation. It outlines context ontop of which the rest of this project is built.

% TODO: explain that ideas will become clearer as they are
%       used and we see how they interact with other concepts
%       introduced later. it's ok to leave concepts not entirely
%       developed

%\section{Formal Systems}
%
%\subsection{Domain Modelling}
%
%\subsubsection{Syntax and Grammars}
%
%\subsection{Derivation Rules}
%\subsection{Derivation Strategies}

\section{\lam-Calculus}

Description of \lam-calculus: what why and who?

\subsection{Syntax}
  
  \lam-variables are represented by $x,y,z,x^\prime,y^\prime,\&c$. Variables 
  denote an arbitrary value: they do not describe what the value is but that 
  any two occurrences of the same variable represent the same value. The 
  grammar for constructing well-formed \lam-terms is:

  \begin{figure}[!h]
  \definition{ 
    \textsc{(Grammar for untyped \lam-calculus)}
    \item $`l$-variables are denoted by $x, y,\dots$ \\
    \[
    \begin{array}{rcl}
    M,N & ::= & x\ \mid\ `lx.M\ \mid\ M\ N
    \end{array}
    \]
  }
  \end{figure}

  \lam-abstractions are represented by $`lx.M$ where $x$ is a parameter and 
  $M$ is the body of the abstraction. The same idea is expressed by more
  conventional notation as a mathematical function $f(x) = M$. The $`l$ 
  annotates the beginning of an abstraction and the $.$ separates the parameter 
  from the body of the abstraction. This syntax is inductive meaning the body 
  is just another term constructed using the same rules. Some examples of 
  abstractions are:
  \begin{figure}[!h]
    \[
      \begin{array}{l}
      `lx.x \\
      `lx.y \\
      `lx.(`ly.xy)
      \end{array}
    \]
  \caption{Examples of valid \lam-abstractions}
  \end{figure}
  
  Applications are represented by any two terms, constructed according to
  the grammar, placed alongside one another. Application gives highest
  precedence to the leftmost terms. Bracketing can be introduced to enforce 
  alternative application order for example $xyz$ is implicitly read as 
  $(xy)z$ but an be written as $x(yz)$ to describe that the application of 
  $yz$ should come first. Examples of applications are:
    \begin{figure}[!h]
      \[
        \begin{array}{l}
        xy \\
        xyz \\
        x(yz) \\
        (`lx.x)y
        \end{array}
      \]
    \caption{Examples of valid applications}
    \end{figure}

\subsection{Reduction Rules}

  First we will introduce the substitution notation $M[N/x]$. This denotes 
  the term M with all occurrences of x replaced by N. The substitution 
  notation is defined inductively as:
   
    \begin{figure}[!h]
    \definition{ 
      \textsc{(Substitution notation for \lam-terms)}
      \[
      \begin{array}{rclr}
      x[y/x] & \rightarrow & y \\
      z[y/x] & \rightarrow & z & (z \neq x) \\
      (`lz.M)[y/x] & \rightarrow & `lz.(M[y/x]) \\
      (M N)[y/x] & \rightarrow & M[y/x] N[y/x]
      \end{array}
      \]
    }
    \end{figure}

  The main derivation rule of the \lam-calculus is \bta-reduction. If term
  $M$ \bta-reduces to term $N$, we write $M \rightarrow_{`b} N$ although 
  the \bta\ subscript can be omitted if it is clear from context. \bta-reduction
  is defined for all \lam-terms:
  \begin{figure}[!h]\label{def:beta-reduction}
  \definition{ 
    \textsc{($`b$-reduction rules for \lam-calculus)}
    \[
    \begin{array}{rcl}
    x & \rightarrow_{`b} & x \\
    `lx.M & \rightarrow_{`b} & `lx.M \\
    (`lx.M) N & \rightarrow_{`b} & M[N/x]
    \end{array}
    \]
  }
  \end{figure}
  It is evident from the rules in Definition \ref{def:beta-reduction} that 
  variables and abstractions \bta-reduce to themselves. Only applications 
  reduce to other terms. This means that an application is a reducible 
  expression or a \emph{redex}. 
  
  Reducing a \emph{redex} models the running of a function. It is 
  \bta-reduction that provides the \lam-calculus with the ability to
  model computation. 

  %TODO: add alpha reduction(?)
  \subsection{Reduction Strategies}

%\section{Logic and Types}
%  \subsection{Logic Systems}
%  \subsubsection{Intuitionistic}
%  \subsubsection{Classical}
%  \subsubsection{Sequent}
%
%  \subsection{Type Assignment}
%
%  \subsection{Curry-Howard Isomophism}
%
%\section{Haskell}
%  % TODO: purity and laziness
%  \subsection{Data Types}
%  \subsection{Type Level/Value Level}
%  \subsection{Term Rewriting}
%  % explain monads as generalizations of cps-terms
%  % what they encode and why they are useful

\section{Continuations}
  
  Compound terms can be decomposed into two seperate parts: a dominant 
  term and a context. The dominant term is the term currently being 
  evaluated. The context is a term with a hole that will be filled with 
  the value the dominant term reduces to.
  
  \begin{figure}[!h]
    \hspace{1cm}Assume that $M \rightarrow_{`b} M^\prime$
    \[
    \begin{array}{lrcl}
    \textit{(Compound term)}&& MN \\
    \textit{(Decompose)}&M && \square N \\
    \textit{(Beta-reduce dominant term)}& M^\prime && \square N \\
    \textit{(Refill hole of context)}&& M^\prime N \\
    \end{array}
    \]
  \caption{Decomposing a term into a dominant term and a context}
  \end{figure}
  
  When $M$ has \bta-reduced to $M^\prime$ then the hole of the context
  $\square N$ is filled to form $M^\prime N$. What the dominant term and
  context are for a given term depends on the reduction rules and strategy. 
  The context is what remains to be reduced at given moment of reduction.
  Thus a context is also called a \emph{continuation}.
 
  \subsection{Undelimited Continuations} 
 
  For more complex terms, the waiting context will grow as the dominant
  term gets further decomposed:
  
  \begin{figure}[!h]
    \[
    \begin{array}{ll}
      (MM^\prime) M^{\prime\prime} \\
      (MM^\prime) & \square M^{\prime\prime} \\
      M & (\square M^\prime) M^{\prime\prime} \\
    \end{array}
    \]
  \caption{Decomposing a term into multiple contexts}
  \end{figure}

  By amalgamating continuatinos into one, big continuation we only have
  two components at point during the reduction: the current dominant term
  and the \emph{current continuation}. 
  
  Assume we have some control operations defined for manipulating
  continuations. With this model, these operations would only be able to 
  control the entire remaining continuation. That is to say we can only 
  return values to the entire continuation and not to arbitrary parts of 
  it. For this reason, continuations that can only be manipulated in their 
  entirety are \textbf{undelimited continuations}.

  \subsection{Delimited-Continuations}
 
  For complex terms, we can instead maintain continuations of continuations:
  
  \begin{figure}[!h]
    \[
    \begin{array}{lll}
      (MM^\prime) M^{\prime\prime} \\
      (MM^\prime) & \square M^{\prime\prime} \\
      M & \square M^\prime & \square M^{\prime\prime} \\
    \end{array}
    \]
  \caption{Decomposing a term into multiple contexts}
  \end{figure}

  Here, we have a stack of remaining continuations. When a dominant term
  has been reduced, the reduct is returned to its corresponding continuation. 
  This newly joined term then becomes the dominant redex. After this new 
  dominant term has been reduced, it will be returned to the next waiting 
  continuation, and so on. Throughout this process, we maintain each 
  continuation separately.
  
  Assume again that we have operations defined for manipulating continuations.
  This model would allow use to control different combinations of continuations.
  The increased granularity of control means we can control not just the
  entire remaining context but parts of it. Thus, continuations of this kind
  are called \textbf{delimited continuations}.

  \subsection{Continuation-Passing Style}
 
  Continuations are what is left of the computation. By rewriting 
  \lam-terms, continuations can be made explicit. All terms must
  be turned into \lam-abstractions of $k$ where $k$ is the continuation
  of a term. $k$ is then called on the result of the term, triggering
  the continuation to take control. This style of writing \lam-terms
  is called continuation-passing style or CPS.
  
  \definition{
    \textsc{(Semantics for continuation-passing style)} 
    \[
    \begin{array}{lrcl}
      x     & = & `lk.kx      \\
      `lx.\tr{M} & = & `lk.k(`lx.\tr{M}) \\
      \tr{M N} & = & `lk.M(`lm.m\tr{N}k)
    \end{array}
    \]
  }
  
  The term that a CPS program terminates on will be of the form
  $`lk.kM$. In order to extract the value, a \emph{final continuation}
  must be provided. Depending on the context, this is likely to be an
  identity function $`lx.x$ or a display operation $`lx.display x$ to
  display the results of the program.
 
  % TODO: explain how order-of-evaluation can be enforced using 
  cps.
 
  \subsection{Monads}
  
  % TODO: intuitively, that mu-terms are expressed easily by continuation 
  %       passing style gives us some idea that they are about control flow
  %       and that they will be easily expressed by monads
 
  Haskell has an infix operator for function application called \mono{\$}.
  The operator has the type signature \mono{(a -> b) -> a -> b}. This
  is the operator for function application. For instance we can take a
  function \mono{addOne :: Int -> Int} and write:
  
  \Verbatimcode
  ($) :: (a -> b) -> a -> b
  intToString :: Int -> String
  ($) intToString :: Int -> String
  ($) intToString 4 :: String
  \end{Verbatim}
  
  If we preapply \mono{\$} to an argument \mono{x :: a} then the resulting
  term will be \mono{(\$ x) :: (a -> b) -> b}. We can read \mono{(\$ x)} as
  a function that takes an argument $f$ and applies $f$ to $x$. It is a term
  waiting for a function. This is the same type as CPS terms:
  
  \[
  \begin{array}{l}
    M : B \\
    `lk.kM : (A \rightarrow B) \rightarrow B
  \end{array}
  \]
 
  A term with type $(A \rightarrow B) \rightarrow B$ represents a suspended
  computation. When the computation is passed a function with which to
  continue, the computation will return a value of type $B$. We will call
  this a $B$-computation or $Comp\ B$ for short. If we have a term of type
  $Comp\ B$, we have a term that requires a function. 
  
  % TODO: finish explanation of linking suspended-computations and CPS
  %       to monads

\section{\lmu-Calculus}
  \subsection{Syntax}
  \begin{figure}[!h]
  \definition{ 
    \textsc{(Grammar for \lmu-calculus)}
    \item $`l$-variables are denoted by $x, y,\dots$ and $`m$-variables are denoted by $`a, `b,\dots$ \\
    \[
    \begin{array}{lrcl}
    
    \text{(Unnamed term)} & M,N & ::= & x\ |\ `lx.M\ |\ M\ N\ |\ `m`a.C \\
    \text{(Named term)} & C & ::= & [`a]M
    \end{array}
    \]
  }
  \end{figure}
  
  Just as \lam\ introduces a new \lam-abstraction, \lmu\ introduces a new 
  \lmu-abstraction. The body of a \lmu-abstraction must be a named term. 
  A named term consists of a name of the form $[`a]$ followed by an unnamed 
  term. 

  \subsection{Reduction Rules}
  \begin{figure}[!h]
  \definition{ 
    \textsc{(Reduction rules for \lmu-calculus)}
    \[
    \begin{array}{rcl}
    x & \rightarrow & x \\
    `lx.M & \rightarrow & `lx.M \\
    `m`a.[`b]M & \rightarrow & `m`a.[`b]M \\
    (`lx.M) N & \rightarrow & M[N/x] \\
    (`m`a.[`b]M) N & \rightarrow & (`m`a.[`b]M[[`g]M^\prime N/[`a]M^\prime]) \\
    \end{array}
    \]
  }
  \end{figure}

  The terse reduction rule at the end simple states that the application
  of a \lmu-abstraction $`m`a.M$ to a term $N$ applies all the sub-terms 
  of $M$ labelled $[`a]$ to $N$ and relabels them with a fresh $`m$ 
  variable.
  
  \subsection{Computational Significance}

  The additional \lmu\ reduction rules model context manipulation. 
  \lmu-variables map to contexts. When an unnamed term is labelled with a 
  \lmu-variable, it is evaluated in that context. For instance the named 
  term $['a]M$ has the effect of evaluating $M$ in the context pointed to 
  by $`a$.
  
  To make this more concrete, consider the compound term $(`m`a.[`b]M)\ N$. 
  First we decompose the term into a dominant term $(`m`a.[`b]M)$ and a 
  context $\square N$. Informally, we can imagine that the \lmu-variable 
  $`a$ now maps to this context $\{`a \Rightarrow \square N\}$:
  
  % TODO: reform to get rid of context mapping:
  %   keep mu abstraction in dominant and consult context for
  %   what to apply named-terms to
  \begin{example}[]
    \[
    \begin{array}{lc}
    \textbf{Dominant} & \textbf{Context} \\
    (`m`a.[`b]M) N \\
    `m`a.[`b]M & \square N \\
    \end{array}
    \]
  \end{example}

  All subterms of $M$ labelled $`a$ will now be evaluated in the context 
  $\square N$ and the context will destroyed. For example, let us replace 
  $M$ with \mbox{$`m\circ.[`a](`ls.fs)$}:
  
  \begin{example}
    \[
    \begin{array}{lcr}
    \textbf{Dominant} & \textbf{Context} \\
    `m`a.[`b]`m\circ.[`a](`ls.fs)    & \square N \\
    `m`a.[`a](`ls.fs)    & \square N \\
    `m`g.[`g](`ls.fs)N   & & (`g\ \text{fresh})  \\
    \end{array}
    \]
  \end{example}

  % TODO: explain applicative contexts
  After applying the term $[`a](`ls.fs)$ to $N$, the context $\square N$
  is consumed and every occurrence of $`a$ is replaced with a fresh 
  variable -- in this case a $`g$ -- to clarify that the new 
  \lmu-abstraction points to a new context. This means that 
  \lmu-abstractions will pass all of the applicative contexts to the
  named subterms:
  
  \begin{example}
    \[
    \begin{array}{lcr}
    \textbf{Dominant} & \textbf{Context} \\
    (`m`a.[`a](`ls.`lt.st)) M N \\
    (`m`a.[`a](`ls.`lt.st))M & \square N \\
    `m`a.[`a](`ls.`lt.st) & \square M:\square N \\
    `m`g.[`g](`ls.`lt.st)M & \square N & (`g\ \text{fresh}) \\
    `m`d.[`d](`ls.`lt.st)MN & \square N & (`d\ \text{fresh}) \\
    \end{array}
    \]
  \end{example}
  
  % \subsection{Reduction Strategies}
  \subsection{Isomorphism \& Computational Interpretation}

\section{\ltry-Calculus}

\section{Delimited-Continuation Calculus}

  Simon Peyton-Jones \textit{et al.}\ extended the \lam-calculus with additional operators in order create a framework for implementing delimited continuations \cite{JonesDS07}. This calculus will be referred to as the delimited-continuation calculus or DCC. Many calculi have been devised with control mechanisms. Like the \lmu-calculus, these control mechanisms are all specific instances of delimited and undelimited continuations. DCC provides a set of operations that are capable of expressing many of these other common control mechanisms.

  The grammar of DCC is an extension of the standard \lam-calculus:

  \subsection{Syntax}
  \begin{figure}[!h]
  \definition{ 
    \textsc{(Grammar for DCC)}
    \[
    \begin{array}{lrcl}
    \textrm{(Variables)} & x, y, \dots \\
    \textrm{(Expressions)} & e & ::= & x\ |\ `lx.e\ |\ e\ e^\prime \\
                           &   &  |  &  newPrompt\ |\ pushPrompt\ e\ e \\
                           &   &  |  &  withSubCont\ e\ e\ |\ pushSubCont\ e\ e
    \end{array}
    \]
  }
  \end{figure}

  \subsection{Reduction Rules}
  The operational semantics can be understood through an abstract machine that transforms tuple of the form $\langle e,\ D,\ E\, q \rangle$:

  \begin{figure}[!h]
  \relscale{0.9}
  \definition{ 
    \textsc{(Operational semantics for DCC)}
    \[
    \begin{array}{lrcl}
      \langle e\ e^\prime, D, E, q \rangle &\Rightarrow &\langle e, D[\square\ e^\prime], E, q \rangle &\text{e non-value} \\
      \langle v\ e, D, E, q \rangle &\Rightarrow &\langle e, D[v\ \square], E, q \rangle &\text{e non-value} \\
      \langle pushPrompt\ e\ e^\prime, D, E, q \rangle &\Rightarrow &\langle e, D[pushPrompt\ \square\ e^\prime], E, q \rangle &\text{e non-value} \\
      \langle withSubCont\ e\ e^\prime, D, E, q \rangle &\Rightarrow &\langle e, D[withSubCont\ \square\ e^\prime], E, q \rangle &\text{e non-value} \\
      \langle withSubCont\ p\ e, D, E, q \rangle &\Rightarrow &\langle e, D[withSubCont\ p\ \square], E, q \rangle &\text{e non-value} \\
      \langle pushSubCont\ e\ e^\prime, D, E, q \rangle &\Rightarrow &\langle e, D[pushSubCont\ \square\ e^\prime], E, q \rangle &\text{e non-value} \\
    \\
      \langle (`lx.e)\ v, D, E, q \rangle &\Rightarrow &\langle e[v/x], D, E, q \rangle \\
      \langle newPrompt, D, E, q \rangle &\Rightarrow &\langle q, D, E, q+1 \rangle \\
      \langle pushPrompt\ p\ e, D, E, q \rangle &\Rightarrow &\langle e, \square, p : D : E, q \rangle \\
      \langle withSubCont \ p\ v, D, E, q \rangle &\Rightarrow &\langle v (D : E\textsmaller[1]{\overset{p}{\uparrow}}, \square, E\textsmaller[1]{\overset{p}{\downarrow}}, q \rangle \\
      \langle pushSubCont E^\prime\ e, D, E, q \rangle &\Rightarrow &\langle e, \square, E^\prime +{+} (D : E), q \rangle \\
    \\
      \langle v, D, E, q \rangle &\Rightarrow &\langle D[v], \square, E, q \rangle \\
      \langle v, \square, p : E, q \rangle &\Rightarrow &\langle v, \square, E, q \rangle \\
      \langle v, \square, D : E, q \rangle &\Rightarrow &\langle v, D, E, q \rangle
    \end{array}
    \]
  }
  \end{figure}
  
  \subsection{Significance}

  % TODO: ensure prompts and continuation stack has been explained before reaching this point
  The additional terms behave as follows:
  \begin{itemize}
  \item \op{newPrompt} returns a new and distinct prompt.
  \item \op{pushPrompt}'s first argument is a prompt which is pushed onto the continuation stack before evaluating its second argument. 
  \item \op{withSubCont} captures the subcontinuation from the most recent occurrence of the first argument (a prompt) on the excution stack to the current point of execution. Aborts this continuation and applies the second argument (a \lam-abstraction) to the captured continuation.
  \item \op{pushSubCont} pushes the current continuation and then its first argument (a subcontinuation) onto the continuation stack before evaluating its second argument.
  \end{itemize}
