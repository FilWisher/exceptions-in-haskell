\chapter{Background}

This chapter explores what \emph{formal systems} are and what they are useful for. It looks at a number of related formal systems and their relation to computation. It outlines the context ontop of which the rest of this project is built.

% TODO: explain that ideas will become clearer as they are
%       used and we see how they interact with other concepts
%       introduced later. it's ok to leave concepts not entirely
%       developed

\section{Formal Systems}

Formal systems are a set of rules for writing and manipulating
formulae. Formulae are constructed from a set of characters called the
\emph{alphabet} by following a some formula-construction rules called
the \emph{grammar}. The only formulae considered \emph{well-formed} in
a system are those constructed according to the grammar of a system.
Formal systems are used to model domains of knowledge to help better
and more formally understand those domains.

\subsection{Syntax and Grammars}

The grammar of a formal system describes the system's syntax. Grammars are 
rules for constructing formulae that are well-formed. Formulae
produced according to the grammar of a system are well-formed according to 
the syntax of that system.

We defined grammars using Backus-Naur Form or BNF:

\[
  M ::= t\ \mid\ f
\]

This grammar describes that syntactically-valid constructs are either
the letter $t$ or the letter $f$. Grammars can be recursive
which allows much more expressive construction rules:

\begin{figure}[!h]\label{fig:tf-grammar}
\[
  \begin{array}{lclr}
    M,N&::=&t&\textcolor{pale-gray}{(1)} \\
      &|&f&\textcolor{pale-gray}{(2)} \\
      &|&M\ a\ N&\textcolor{pale-gray}{(3)} \\
      &|&M\ o\ N&\textcolor{pale-gray}{(4)}
  \end{array}
\]
\caption{Grammar for producing the letters $t$ or $f$ connected by the letters $a$ or $o$}
\end{figure}

This grammar describes formulae containing the any number of occurrences of
the letters $t$ or $f$ separated by either an $a$ or an $o$.

\[
\begin{array}{lr}
  \by{t}{1} \\
  \by{t\ o\ f}{2 \& 4} \\
  \by{t\ o\ f\ a\ t}{1 \& 3}
\end{array}
\]

\subsection{Derivation Rules}
Whereas a grammar describes the rules for producing well-formed formulae,
the derivation rules describe rules for transforming formulae of a
particular form into a new formula. Using the grammar from Figure 
\ref{fig:tf-grammar}, we add derivation rules:

\[
\begin{array}{lcl}
  t\ a\ M &\to& M \\ 
  M\ a\ t &\to& M \\
  f\ a\ f &\to& f \\
  \\
  M\ o\ t &\to& t \\ 
  t\ o\ M &\to& t \\
  f\ o\ f &\to& f \\
\end{array}
\]

These rules describe that if a formula matches the pattern on the 
left-hand side, where $M$ represents a well-formed formula, it can be
replaced by the formula on the right-hand side.

\subsection{Domain Modelling}

The syntax and derivation rules of a formal system are defined to model 
some domain. This isomorphism between the domain and the formal system
means we can attempt to discover truths about the domain through studying
the formal system.

For example, take the $tf$-system described above. Without
understanding the domain, we are able to manipulate formulae of the
system to create new formulae. The $tf$-system is isomorphic to
Boolean algebra:

\[
\begin{array}{cc}
\text{$tf$-system} & \text{Boolean algebra} \\
t & 1 \\
t & 0 \\
a & \wedge \\
o & \vee 
\end{array}
\]

Using formal systems allows us to understand the domains they model from
different perspectives and thereby learn novel truths about them.

%\subsection{Derivation Strategies}
% TODO: explain that when you can make more than one decision,
%       you can have a strategy for which decision you take

\section{\lam-Calculus}

The \lam-calculus is a formal system described by Alonzo Church. The system
models the execution of computer programs. As a Turing-complete system,
it is capable of expressing the solutions to all problems that can
be solved by a computer. It is of interest for this project because we
can use it to model computer programs.

\subsection{Syntax}
  
  \lam-variables are represented by $x,y,z,\&c$. Variables denote an
  arbitrary value: they do not describe what the value is but that any
  two occurrences of the same variable represent the same value. The
  grammar for constructing well-formed \lam-terms is:

  \begin{figure}[!h]
  \definition{ 
    \textsc{(Grammar for untyped \lam-calculus)}
    \item $`l$-variables are denoted by $x, y,\dots$ \\
    \[
    \begin{array}{rcl}
    M,N & ::= & x\ \mid\ `lx.M\ \mid\ M\ N
    \end{array}
    \]
  }
  \end{figure}

  \lam-abstractions are represented by $`lx.M$ where $x$ is a
  parameter and $M$ is the body of the abstraction. The same idea is
  expressed by more conventional notation as a mathematical function
  $f(x) = M$. The $`l$ annotates the beginning of an abstraction and
  the $.$ separates the parameter from the body of the abstraction.
  This grammar is recursive meaning the body of an abstraction is just
  another term constructed according to the grammar. Some examples of
  abstractions are:
  
  \begin{figure}[!h]
    \[
      \begin{array}{l}
      `lx.x \\
      `lx.xy \\
      `lx.(`ly.xy)
      \end{array}
    \]
  \caption{Examples of valid \lam-abstractions}
  \end{figure}
  
  Applications are represented by any two terms, constructed according to
  the grammar, placed alongside one another. Application gives highest
  precedence to the left-most terms. Bracketing can be introduced to enforce 
  alternative application order for example $xyz$ is implicitly read as 
  $(xy)z$ but an be written as $x(yz)$ to describe that the application of 
  $yz$ should come first. Examples of applications are:
    \begin{figure}[!h]
      \[
        \begin{array}{l}
        xy \\
        xyz \\
        x(yz) \\
        (`lx.x)y
        \end{array}
      \]
    \caption{Examples of valid applications}
    \end{figure}

\subsection{Reduction Rules}

  First we will introduce the substitution notation $M[N/x]$. This denotes 
  the term M with all occurrences of x replaced by N. The substitution 
  notation is defined inductively as:
   
    \begin{figure}[!h]
    \definition{ 
      \textsc{(Substitution notation for \lam-terms)}
      \[
      \begin{array}{rclr}
      x[y/x] & \rightarrow & y \\
      z[y/x] & \rightarrow & z & (z \neq x) \\
      (`lz.M)[y/x] & \rightarrow & `lz.(M[y/x]) \\
      (M N)[y/x] & \rightarrow & M[y/x] N[y/x]
      \end{array}
      \]
    }
    \end{figure}

\subsubsection{\bta-reduction}
  The main derivation rule of the \lam-calculus is \bta-reduction. If term
  $M$ \bta-reduces to term $N$, we write $M \rightarrow_{`b} N$ although 
  the \bta\ subscript can be omitted if it is clear from context. \bta-reduction
  is defined for the application of two terms:
  \begin{figure}[!h]\label{def:beta-reduction}
  \definition{ 
    \textsc{($`b$-reduction for \lam-calculus)}
    \[
    \begin{array}{rcl}
    (`lx.M) N & \rightarrow_{`b} & M[N/x]
    \end{array}
    \]
  }
  \end{figure}
 
  \lam-variables and \lam-abstactions are \emph{values}: they do not reduce
  to other terms. If a formula is a value, the reduction terminates on that 
  value. Only applications reduce to other terms. This means that an 
  application is a reducible expression or a \emph{redex}. Reducing a 
  redex models the computing of a function. 

% TODO: define free variables and bound variables
\subsubsection{$\alpha$-reduction}
  
  The \lam-calculus defines a reduction rule for renaming variables.
  Variable names are arbitrary and chosen just to denote identity:
  all occurrences of $x$ are the same. This can become a problem
  in the following case:
  
  \[
    (`ly.`lx.xy)(`lx.x)
  \]
 
  After the application is reduced, we have the term:
  
  \[
    `lx.x(`lx.x) 
  \]
  
  In this case, it is ambiguous which \lam-abstraction the right-most $x$
  is bound by. When this term is applied to another, will the substitution
  occur to all occurrences of $x$? From the initial term, it is clear that
  this would be incorrect. The \lam-calculus introduces $`a$-reduction to
  solve this:
  
  \begin{figure}[!h]\label{def:alpha-reduction}
  \definition{ 
    \textsc{($`a$-reduction for \lam-calculus)}
  \[
    \begin{array}{rl}
    `lx.M \rightarrow_{`a} `ly.M[y/x] & (y \notin fv(M))
    \end{array}
  \]
  }
  \end{figure}
  
  This means we can rename the lead variable of an abstraction $M$ on the 
  conditions that: 
  \begin{enumerate}
    \item All variables bound by that abstraction are renamed the same 
    \item The variable it is changed to is not currently in use in $M$ 
  \end{enumerate}
  
  
  %TODO: add alpha reduction(?)
  % \subsection{Reduction Strategies}

\section{Logic and Types}
%  \subsection{Logic Systems}
  \subsection{Implicative Intuitionistic Logic}
  A formal system based on Gerhard Gentzen's natural deduction.
  Restricted only to $\rightarrow \mathcal{I}$ and $\rightarrow 
  \mathcal{E}$.
  Following Brouwer, also leaves out law of excluded middle.
  
  \[
  \begin{array}{rl@{\quad\quad}rl}
    ( \to \mathcal{I} ) &
    \Inf { A \to B \hspace{0.5cm} A }
        {B}
    &
    ( \to \mathcal{E} ) &
    \Inf { [A] \vdash B }
        { A \to B}
  \end{array}
  \]
  
%  \subsubsection{Classical}
%  \subsubsection{Sequent}
%
  \subsection{Type Assignment}
  
  Type assignment introduces additional grammar and restrictions on
  the reduction rules of a system. These extensions prevent logically
  inconsistent terms from being constructed. A type assignment has the
  form:
  
  \[
    M : `a 
  \]
  
  which states that term $M$ has the type $`a$. Like variables, type 
  variables are abstract: they do not describe anything more about a 
  type than its identity. That is to say $x: A$ and $y : A$ have the
  same type but we cannot say any more about what that type is.
 
  A type is either some uppercase Latin letter or it is two valid types
  connected by a $\rightarrow$. This is described by the following
  BNF grammar:
  
  \[
    A,B ::= `v \mid A \rightarrow B 
  \]
  
  Typing rules have their own form that resembles Gentzen's sequent 
  calculus:
  
  \[
    `G \vdash T 
  \]
    
  This describes that $`G$ is a set of typed \lam-terms and $T$ is a
  typed \lam-term that is derivable from $`G$.

  \subsection{Curry-Howard Isomophism}
  
  Curry-Howard isomorphism states that their is an isomorphism between
  the typed of a term and a logical proposition. The term itself is
  the proof of the proposition.
  
  % TODO: add more - examples and demonstration

\section{Haskell}
  % TODO: describe purity and laziness
  
  \subsection{Data Types}
  New data types can be introduced into Haskell in 3 distinct ways. First,
  using the \mono{data} keyword:
  
  \Verbatimcode
    data Animal a = Dog a
      | Cat
  \end{Verbatim}
  
  The \mono{data} keyword begins the definition of a new data type. The
  word immediately following determines the type constructor for the new
  type. Following this is a type parameter for the type constructor. There
  can be any number of type parameters, including zero. The right-hand
  side of the \mono{=} introduces a \mono{|}-separated list of data 
  constructors.
  
  \Verbatimcode
    > let hector = Cat
    > :t hector
    hector :: Animal a
    
    > let topaz = Dog "foo"
    > :t topaz
    topaz :: Animal String
  \end{Verbatim}
  
  The type parameter is constrained by the type of the value the data
  constructor was initialized with. In the example above, calling the
  \mono{Dog} data constructor with a string makes the type \mono{Animal
  String} rather than the more general \mono{Animal a}.
  
  The second method for introducing new data types is the \mono{newtype}
  keyword. The key difference between \mono{data} and \mono{newtype} is
  that \mono{newtype} can only have one data constructor. Informally,
  this implies a kind of isomorphism:

  \Verbatimcode
    newtype Foo a = Foo (a -> Integer)
  \end{Verbatim}
  
  The type constructor can take type parameters which will be constrained
  by the inhabitants of the data constructor. This data type expresses
  an isomorphism between \mono{Foo a} and functions from \mono{a} to
  \mono{Integer}s.
  
  Finally, we can introduce type aliases using the \mono{type} keyword:
  \Verbatimcode
    type Name = String 
  \end{Verbatim}
  
  Again, we introduce a type constructor \mono{Name} but this time we
  name another type, in this case \mono{String}, as its inhabitant. This
  means that the type \mono{Name} is a type alias for \mono{String} and
  will share the same data constructors. 
 
  \subsection{Type Level/Value Level}
  Haskell distinguishes between terms on the type level and terms on the
  value level. Type level terms are descriptions of the types of a value.
  They provide restrictions on the construction of invalid terms. For
  instance if we have a function of type \mono{String -> Integer}, we cannot
  apply it to a term of type \mono{Boolean}. The type-checker will throw
  an error before any value-level computation is initiated.
 
  % TODO: better description needed
  The value level is the level on which data is constructed and manipulated.
  The operation \mono{1+1} occurs on the value level. The value level is
  where computation takes place and the type level is where static analysis
  of the program type takes place.
  
  \subsection{Type Classes}
  Haskell adds type classes to the type level. Types can have instances of
  type classes. The most similar concept from Object-Oriented programming
  is \emph{interfaces}.
  
  \Verbatimcode
    class Addable a where
      (add) :: a -> a -> a
  \end{Verbatim}
  
  Type classes are introduced using the \mono{class} keyword. Beneath that
  are the function names and corresponding type-signatures of the functions
  that an instance of a class must implement.
  
  For example, we can create instances of the \mono{Addable} class:
  
  \Verbatimcode
    data Number = One | Two | ThreeOrMore
    
    instance Addable Number where
      add One One = Two 
      add One Two = ThreeOrMore 
      add Two One = ThreeOrMore 
  \end{Verbatim}    
  
  %\subsection{Term Rewriting}
  % explain ($) operator
  % explain monads as generalizations of cps-terms
  % what they encode and why they are useful

% TODO: these explanations of continuations (diff between delim and 
%       undelim) are incorrect 
\section{Continuations}
  
  Compound terms can be decomposed into two seperate parts: a dominant 
  term and a context. The dominant term is the term currently being 
  evaluated. The context is a term with a hole that will be filled with 
  the value the dominant term reduces to.
  
  \begin{figure}[!h]
    \hspace{1cm}Assume that $M \rightarrow_{`b} M^\prime$
    \[
    \begin{array}{lrcl}
    \textit{(Compound term)}&& MN \\
    \textit{(Decompose)}&M && \square N \\
    \textit{(Beta-reduce dominant term)}& M^\prime && \square N \\
    \textit{(Refill hole of context)}&& M^\prime N \\
    \end{array}
    \]
  \caption{Decomposing a term into a dominant term and a context}
  \end{figure}
  
  When $M$ has \bta-reduced to a value -- $M^\prime$ -- then the hole
  of the context $\square N$ is filled to form $M^\prime N$. What the
  dominant term and context are for a given term depends on the
  reduction rules and strategy. The context is what remains to be
  reduced at given moment of reduction. Thus a context is also called
  a \emph{continuation}.
 
  \subsection{Undelimited Continuations} 
 
  For more complex terms, the waiting context will grow as the dominant
  term gets further decomposed:
  
  \begin{figure}[!h]
    \[
    \begin{array}{ll}
      (MM^\prime) M^{\prime\prime} \\
      (MM^\prime) & \square M^{\prime\prime} \\
      M & (\square M^\prime) M^{\prime\prime} \\
    \end{array}
    \]
  \caption{Decomposing a term into multiple contexts}
  \end{figure}

  By amalgamating continuatinos into one big continuation we only have
  two components at point during the reduction: the current dominant term
  and the \emph{current continuation}. 
  
  Assume we have some reduction rules defined for manipulating
  continuations. This model keeps continuations grouped together which
  means these hypothetical reduction rules could manipulate only this
  entire remaining continuation. For this reason, continuations that
  can only be manipulated in their entirety are \textbf{undelimited
  continuations}.

  \subsection{Delimited-Continuations}

  Instead, if we maintain a stack of continuations when
  decomposing complex terms, we can keep continuations separated:
  
  \begin{figure}[!h]
    \[
    \begin{array}{lll}
      (MM^\prime) M^{\prime\prime} \\
      (MM^\prime) & \square M^{\prime\prime} \\
      M & \square M^\prime & \square M^{\prime\prime} \\
    \end{array}
    \]
  \caption{Decomposing a term into multiple contexts}
  \end{figure}

  Here, when a dominant term has been reduced, the reduct is returned
  to its corresponding continuation. This newly joined term then
  becomes the dominant redex. After this new dominant term has been
  reduced, it will be returned to the next waiting continuation, and
  so on. Throughout this process, we maintain each continuation
  separately.
  
  Assume again that we have reduction rules defined for manipulating
  continuations. By keeping continuations separate, this model would 
  allow use to use parts of the stack selectively for instance placing
  delimiters between portions of interest. The increased granularity of 
  control means we can manipulate not just the entire remaining continuation 
  but sections of it. Thus, continuations of this kind are called 
  \textbf{delimited continuations}.

  \subsection{Continuation-Passing Style}
 
  By rewriting \lam-terms, the continuations can be made explicit. All
  terms must be turned into \lam-abstractions of some variable $k$
  where $k$ is the continuation of a term. $k$ is then called on the
  result of the term, triggering the continuation to take control.
  This style of writing \lam-terms is called continuation-passing
  style or CPS.
  
  \definition{
    \textsc{(Translation of standard \lam-terms into CPS)} 
    \[
    \begin{array}{lcl}
      \tr{x}     & = & `lk.kx      \\
      \tr{`lx.M} & = & `lk.k(`lx.\tr{M}) \\
      \tr{M N} & = & `lk.M(`lm.m\tr{N}(`ln.mnk)
    \end{array}
    \]
  }
  
  The term that a CPS program terminates on will be of the form
  $`lk.kM$. In order to extract the value, a \emph{final continuation}
  must be provided. Depending on the context, this could be an identity 
  function $`lx.x$ or a display operation $`lx.\text{display }x$ to
  display the results of the program.

  \begin{figure}[!h]
  \caption{Extracting the final value from a terminated CPS program}
  \[
  \begin{array}{ll}
                & (`lk.kM)(`lx.x) \\
    \rightarrow & (`lx.x)M \\
    \rightarrow & M
  \end{array}
  \]
  \end{figure}
 
  The translation of standard \lam-terms into CPS similarly transforms the 
  \emph{types} of \lam-terms. For example, a term $x : A$ becomes 
  $`lk.kx : (A \to B) \to B$. This type represents a delayed computation:
  a computation that is waiting for a function to continue execution with. 
  In order to resume the computation, the term must be applied to a 
  continuation.
  
  As an example, take the term M where
  
  \[
    M = `lk.kx
  \]

  To access the value $x$ contained in $M$, we have to apply $M$ to a
  continuation function $`lm.N$:
  
  \[
  \begin{array}{l}
      (`lk.kx)(`lm.N) \\
      (`lm.N)x \\
      N[x/m]
  \end{array}
  \]
  
  Within the body of $N$, $m$ is bound to the value contained by $M$.
  So we can think about $M$ as a suspended computation that, when applied
  to a continuation, applies the continuation to $x$. Looking at
  the type $(A \to B) \to B$ again, it is clear that $A$ is the type
  of the term passed to the continuation of a CPS-term:
  
  \[
  \begin{array}{l}
    `lk.kx : (A \to B) \to B \\
    k : (A \to B) \\
    kx : B \\
    x : A \\
  \end{array} 
  \]
  
  % TODO: explain how order-of-evaluation can be enforced using 
  %       cps.
 
  \subsection{Monads}
  
  If we have two suspended computations $M$ and $M^\prime$ and we want to
  run $M$ and then $M^\prime$, we have to apply $M$ to a continuation
  to access its value and then do the same to $M^\prime$:
  
  \[
    M(`lm.M^\prime(`lm^\prime.N))
  \]
  
  This is a common operation so we define a utility operator \mono{>>=}
  that binds the first suspended computation to a continuation which 
  returns another suspended computation\footnote{ \mono{(>>=)} is pronounced 'bind'.}:
 
  \[
    \mono{>>=} : ((A \to B) \to B) \to (A \to ((B \to C) \to C)) \to ((B \to C) \to C)
  \]
 
  The type $(A \to B) \to B$ that represents a suspended computation returning
  a value of type $A$ to its continuation we will call an $A$-computation or
  $Comp\ A$. We can rewrite the type signature of \mono{>>=}:
  
  \[
    \mono{>>=} : Comp\ A \to (A \to Comp\ B) \to Comp\ B
  \]
  
  We define another operator, $return$, that takes a value and returns a 
  suspended computation that returns that value:
  
  \[
    return : A \to Comp\ A 
  \]
 
  The type constructor $Comp\ A$, together with the two utility functions
  \mono{>>=} and $return$, make up Haskell's Monad type class:
  
  \Verbatimcode
    class Monad M where
      (>>=) :: M a -> (a -> M b) -> M b
      return :: a -> M a
  \end{Verbatim}
  
  The Monad type class generalizes CPS terms: they represent suspended
  computations that can be composed using \mono{>>=}. Just like CPS terms,
  a Monad type \mono{M a} tells us that we have a term that will pass
  values of type \mono{a} to the continuation it is bound to using
  \mono{>>=}.
  
  % TODO: finish explanation of linking suspended-computations and CPS
  %       to monads

\section{\lmu-Calculus}
  % TODO: intuitively, that mu-terms are expressed easily by continuation 
  %       passing style gives us some idea that they are about control flow
  %       and that they will be easily expressed by monads
  \subsection{Syntax}
  \begin{figure}[!h]
  \definition{ 
    \textsc{(Grammar for \lmu-calculus)}
    \item $`l$-variables are denoted by $x, y,\dots$ and $`m$-variables are denoted by $`a, `b,\dots$ \\
    \[
    \begin{array}{lrcl}
    
    \text{(Unnamed term)} & M,N & ::= & x\ |\ `lx.M\ |\ M\ N\ |\ `m`a.C \\
    \text{(Named term)} & C & ::= & [`a]M
    \end{array}
    \]
  }
  \end{figure}
  
  Just as \lam\ introduces a new \lam-abstraction, \lmu\ introduces a new 
  \lmu-abstraction. The body of a \lmu-abstraction must be a named term. 
  A named term consists of a name of the form $[`a]$ followed by an unnamed 
  term. 

  \subsection{Reduction Rules}
  \begin{figure}[!h]
  \definition{ 
    \textsc{(Reduction rules for \lmu-calculus)}
    \[
    \begin{array}{rcl}
    x & \rightarrow & x \\
    `lx.M & \rightarrow & `lx.M \\
    `m`a.[`b]M & \rightarrow & `m`a.[`b]M \\
    (`lx.M) N & \rightarrow & M[N/x] \\
    (`m`a.[`b]M) N & \rightarrow & (`m`a.[`b]M[[`g]M^\prime N/[`a]M^\prime]) \\
    \end{array}
    \]
  }
  \end{figure}

  The terse reduction rule at the end simple states that the application
  of a \lmu-abstraction $`m`a.M$ to a term $N$ applies all the sub-terms 
  of $M$ labelled $[`a]$ to $N$ and relabels them with a fresh $`m$ 
  variable.
  
  \subsection{Computational Significance}

  % TODO: use cps translation to explain mu

  The additional \lmu\ reduction rules model context manipulation. 
  \lmu-variables map to contexts. When an unnamed term is labelled with a 
  \lmu-variable, it is evaluated in that context. For instance the named 
  term $['a]M$ has the effect of evaluating $M$ in the context pointed to 
  by $`a$.
  
  To make this more concrete, consider the compound term $(`m`a.[`b]M)\ N$. 
  First we decompose the term into a dominant term $(`m`a.[`b]M)$ and a 
  context $\square N$. Informally, we can imagine that the \lmu-variable 
  $`a$ now maps to this context $\{`a \Rightarrow \square N\}$:
  
  % TODO: reform to get rid of context mapping:
  %   keep mu abstraction in dominant and consult context for
  %   what to apply named-terms to
  \begin{example}[]
    \[
    \begin{array}{lc}
    \textbf{Dominant} & \textbf{Context} \\
    (`m`a.[`b]M) N \\
    `m`a.[`b]M & \square N \\
    \end{array}
    \]
  \end{example}

  All subterms of $M$ labelled $`a$ will now be evaluated in the context 
  $\square N$ and the context will destroyed. For example, let us replace 
  $M$ with \mbox{$`m\circ.[`a](`ls.fs)$}:
  
  \begin{example}
    \[
    \begin{array}{lcr}
    \textbf{Dominant} & \textbf{Context} \\
    `m`a.[`b]`m\circ.[`a](`ls.fs)    & \square N \\
    `m`a.[`a](`ls.fs)    & \square N \\
    `m`g.[`g](`ls.fs)N   & & (`g\ \text{fresh})  \\
    \end{array}
    \]
  \end{example}

  % TODO: explain applicative contexts
  After applying the term $[`a](`ls.fs)$ to $N$, the context $\square N$
  is consumed and every occurrence of $`a$ is replaced with a fresh 
  variable -- in this case a $`g$ -- to clarify that the new 
  \lmu-abstraction points to a new context. This means that 
  \lmu-abstractions will pass all of the applicative contexts to the
  named subterms:
  
  \begin{example}
    \[
    \begin{array}{lcr}
    \textbf{Dominant} & \textbf{Context} \\
    (`m`a.[`a](`ls.`lt.st)) M N \\
    (`m`a.[`a](`ls.`lt.st))M & \square N \\
    `m`a.[`a](`ls.`lt.st) & \square M:\square N \\
    `m`g.[`g](`ls.`lt.st)M & \square N & (`g\ \text{fresh}) \\
    `m`d.[`d](`ls.`lt.st)MN & \square N & (`d\ \text{fresh}) \\
    \end{array}
    \]
  \end{example}
  
  % \subsection{Reduction Strategies}
  \subsection{Isomorphism \& Computational Interpretation}

\section{\ltry-Calculus}

\section{Delimited-Continuation Calculus}

  Simon Peyton-Jones \textit{et al.}\ extended the \lam-calculus with additional operators in order create a framework for implementing delimited continuations \cite{JonesDS07}. This calculus will be referred to as the delimited-continuation calculus or DCC. Many calculi have been devised with control mechanisms. Like the \lmu-calculus, these control mechanisms are all specific instances of delimited and undelimited continuations. DCC provides a set of operations that are capable of expressing many of these other common control mechanisms.

  The grammar of DCC is an extension of the standard \lam-calculus:

  \subsection{Syntax}
  \begin{figure}[!h]
  \definition{ 
    \textsc{(Grammar for DCC)}
    \[
    \begin{array}{lrcl}
    \textrm{(Variables)} & x, y, \dots \\
    \textrm{(Expressions)} & e & ::= & x\ |\ `lx.e\ |\ e\ e^\prime \\
                           &   &  |  &  newPrompt\ |\ pushPrompt\ e\ e \\
                           &   &  |  &  withSubCont\ e\ e\ |\ pushSubCont\ e\ e
    \end{array}
    \]
  }
  \end{figure}

  \subsection{Reduction Rules}
  The operational semantics can be understood through an abstract machine that transforms tuple of the form $\langle e,\ D,\ E\, q \rangle$:

  \begin{figure}[!h]
  \relscale{0.9}
  \definition{ 
    \textsc{(Operational semantics for DCC)}
    \[
    \begin{array}{lrcl}
      \langle e\ e^\prime, D, E, q \rangle &\Rightarrow &\langle e, D[\square\ e^\prime], E, q \rangle &\text{e non-value} \\
      \langle v\ e, D, E, q \rangle &\Rightarrow &\langle e, D[v\ \square], E, q \rangle &\text{e non-value} \\
      \langle pushPrompt\ e\ e^\prime, D, E, q \rangle &\Rightarrow &\langle e, D[pushPrompt\ \square\ e^\prime], E, q \rangle &\text{e non-value} \\
      \langle withSubCont\ e\ e^\prime, D, E, q \rangle &\Rightarrow &\langle e, D[withSubCont\ \square\ e^\prime], E, q \rangle &\text{e non-value} \\
      \langle withSubCont\ p\ e, D, E, q \rangle &\Rightarrow &\langle e, D[withSubCont\ p\ \square], E, q \rangle &\text{e non-value} \\
      \langle pushSubCont\ e\ e^\prime, D, E, q \rangle &\Rightarrow &\langle e, D[pushSubCont\ \square\ e^\prime], E, q \rangle &\text{e non-value} \\
    \\
      \langle (`lx.e)\ v, D, E, q \rangle &\Rightarrow &\langle e[v/x], D, E, q \rangle \\
      \langle newPrompt, D, E, q \rangle &\Rightarrow &\langle q, D, E, q+1 \rangle \\
      \langle pushPrompt\ p\ e, D, E, q \rangle &\Rightarrow &\langle e, \square, p : D : E, q \rangle \\
      \langle withSubCont \ p\ v, D, E, q \rangle &\Rightarrow &\langle v (D : E\textsmaller[1]{\overset{p}{\uparrow}}, \square, E\textsmaller[1]{\overset{p}{\downarrow}}, q \rangle \\
      \langle pushSubCont E^\prime\ e, D, E, q \rangle &\Rightarrow &\langle e, \square, E^\prime +{+} (D : E), q \rangle \\
    \\
      \langle v, D, E, q \rangle &\Rightarrow &\langle D[v], \square, E, q \rangle \\
      \langle v, \square, p : E, q \rangle &\Rightarrow &\langle v, \square, E, q \rangle \\
      \langle v, \square, D : E, q \rangle &\Rightarrow &\langle v, D, E, q \rangle
    \end{array}
    \]
  }
  \end{figure}
  
  \subsection{Significance}

  % TODO: ensure prompts and continuation stack has been explained before reaching this point
  The additional terms behave as follows:
  \begin{itemize}
  \item \op{newPrompt} returns a new and distinct prompt.
  \item \op{pushPrompt}'s first argument is a prompt which is pushed onto the continuation stack before evaluating its second argument. 
  \item \op{withSubCont} captures the subcontinuation from the most recent occurrence of the first argument (a prompt) on the excution stack to the current point of execution. Aborts this continuation and applies the second argument (a \lam-abstraction) to the captured continuation.
  \item \op{pushSubCont} pushes the current continuation and then its first argument (a subcontinuation) onto the continuation stack before evaluating its second argument.
  \end{itemize}
