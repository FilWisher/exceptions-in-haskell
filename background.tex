% TODO: introduce notation and concept of fv/bv and fn/bn

\chapter{Background}

This chapter explores what \emph{formal systems} are and what they are useful for. It looks at a number of related formal systems and their relation to computation. It outlines the context ontop of which the rest of this project is built.

% TODO: explain that ideas will become clearer as they are
%       used and we see how they interact with other concepts
%       introduced later. it's ok to leave concepts not entirely
%       developed

\section{Formal Systems}

Formal systems are a set of rules for writing and manipulating
formulae. Formulae are constructed from a set of characters called the
\emph{alphabet} by following a some formula-construction rules called
the \emph{grammar}. The only formulae considered \emph{well-formed} in
a system are those constructed according to the grammar of a system.
Formal systems are used to model domains of knowledge to help better
and more formally understand those domains.

\subsection{Syntax and Grammars}

The grammar of a formal system describes the system's syntax. Grammars are 
rules for constructing formulae that are well-formed. Formulae
produced according to the grammar of a system are well-formed according to 
the syntax of that system.

We defined grammars using Backus-Naur Form or BNF:

\[
  M ::= t\ \mid\ f
\]

This grammar describes that syntactically-valid constructs are either
the letter $t$ or the letter $f$. Grammars can be recursive
which allows much more expressive construction rules:

\begin{figure}[!h]\label{fig:tf-grammar}
\[
  \begin{array}{lclr}
    M,N&::=&t&\textcolor{pale-gray}{(1)} \\
      &|&f&\textcolor{pale-gray}{(2)} \\
      &|&M\ a\ N&\textcolor{pale-gray}{(3)} \\
      &|&M\ o\ N&\textcolor{pale-gray}{(4)}
  \end{array}
\]
\caption{Grammar for producing the letters $t$ or $f$ connected by the letters $a$ or $o$}
\end{figure}

This grammar describes formulae containing the any number of occurrences of
the letters $t$ or $f$ separated by either an $a$ or an $o$.

\[
\begin{array}{lr}
  \by{t}{1} \\
  \by{t\ o\ f}{2 \& 4} \\
  \by{t\ o\ f\ a\ t}{1 \& 3}
\end{array}
\]

\subsection{Derivation Rules}
Whereas a grammar describes the rules for producing well-formed formulae,
the derivation rules describe rules for transforming formulae of a
particular form into a new formula. Using the grammar from Figure 
\ref{fig:tf-grammar}, we add derivation rules:

\[
\begin{array}{lcl}
  t\ a\ M &\to& M \\ 
  M\ a\ t &\to& M \\
  f\ a\ f &\to& f \\
  \\
  M\ o\ t &\to& t \\ 
  t\ o\ M &\to& t \\
  f\ o\ f &\to& f \\
\end{array}
\]

These rules describe that if a formula matches the pattern on the 
left-hand side, where $M$ represents a well-formed formula, it can be
replaced by the formula on the right-hand side.

\subsection{Domain Modelling}

The syntax and derivation rules of a formal system are defined to model 
some domain. This isomorphism between the domain and the formal system
means we can attempt to discover truths about the domain through studying
the formal system.

For example, take the $tf$-system described above. Without
understanding the domain, we are able to manipulate formulae of the
system to create new formulae. The $tf$-system is isomorphic to
Boolean algebra:

\[
\begin{array}{cc}
\text{$tf$-system} & \text{Boolean algebra} \\
t & 1 \\
f & 0 \\
a & \wedge \\
o & \vee 
\end{array}
\]

Using formal systems allows us to understand the domains they model from
different perspectives and thereby learn novel truths about them.

\subsection{Derivation Strategies}
% TODO: explain that when you can make more than one decision,
%       you can have a strategy for which decision you take

When applying derivation rules to compound terms, we can imagine the 
compound term being decomposed into simpler terms until one of the 
derivation rules applies. When we decompose a term, we separate it into a
dominant term and a context:

\[
\begin{array}{lcr}
  & t\ o\ f\ a\ t \\
  t\ o\ f\ && \square\ a\ t 
\end{array}
\]

The left-hand side is the dominant term and the right-hand side is the context. 
The $\square$ in the context denotes a hole that needs to be filled to create a full term.

Once we have a term that we can apply a derivation rule to, 
we apply the derivation rule and recombine the context with the resulting term:

\[
\begin{array}{rlcr}
      & t\ o\ f\ && \square\ a\ t  \\
  \to & t && \square\ a\ t  \\
  \textit{recombine} & t\ a\ t 
\end{array}
\]

The term $t\ o\ f\ a\ t$ can be decomposed in two ways:

\[
\begin{array}{rlr}
  1. & t\ o\ f\ & \square\ a\ t  \\
  2. & f\ a\ t & t\ o\ \square  \\
\end{array}
\]

When we have more than one way derivation rules can be applied to a term,
we can use \emph{derivation strategies} to determine which rule we apply.
The derivation strategy we use decides how our compound terms are decomposed.
In our $tf$-system, there are two obvious derivation strategies:
either apply derivations starting from the left or starting from the right.

\[
\begin{array}{rcl}
  
  \textbf{(left)} & t\ o\ f\ a\ t \\
  \to         & t\ o\ f & \square\ a\ t \\
  \to         & t       & \square\ a\ t \\
  \to         & t\ a\ t & \\
  \\
  \textbf{(right)}   & t\ o\ f\ a\ t \\
  \to                & f\ a\ t & t\ o\ \square \\
  \to                & f       & t\ o\ \square \\
  \textit{recombine} & t\ o\ f & \\
  
\end{array}
\]
Whereas derivation rules are defined in the system, 
derivation strategies are methods of choosing which derivation rule to apply when given a choice. 

\section{\lam-Calculus}

In response to Hilbert's \emph{Entscheidungsproblem}, Alonzo Church
defined the \lam-calculus. It is a formal system capable of expressing
the set of effectively-computible algorithms. Ontop of this, he built
his proof that not all algorithms are decidable. Shortly after, Godel
and Turing created their own models of effective computibility.
\footnote{General recursive functions and Turing machines, respectively} 
These models were later proved to all be equivalent.

\subsection{Syntax}
  
  \lam-variables are represented by $x,y,z,\&c$. Variables denote an
  arbitrary value: they do not describe what the value is but that any
  two occurrences of the same variable represent the same value. The
  grammar for constructing well-formed \lam-terms is:

  \begin{figure}[!h]
  \definition{ 
    \textsc{(Grammar for untyped \lam-calculus)}
    \item $`l$-variables are denoted by $x, y,\dots$ \\
    \[
    \begin{array}{rcl}
    M,N & ::= & x\ \mid\ `lx.M\ \mid\ M\ N
    \end{array}
    \]
  }
  \end{figure}

  \lam-abstractions are represented by $`lx.M$ where $x$ is a
  parameter and $M$ is the body of the abstraction. The same idea is
  expressed by more conventional notation as a mathematical function
  $f(x) = M$. The $`l$ annotates the beginning of an abstraction and
  the $.$ separates the parameter from the body of the abstraction.
  This grammar is recursive meaning the body of an abstraction is just
  another term constructed according to the grammar. Some examples of
  abstractions are:
  
  \begin{figure}[!h]
    \[
      \begin{array}{l}
      `lx.x \\
      `lx.xy \\
      `lx.(`ly.xy)
      \end{array}
    \]
  \caption{Examples of valid \lam-abstractions}
  \end{figure}
  
  Applications are represented by any two terms, constructed according to
  the grammar, placed alongside one another. Application gives highest
  precedence to the left-most terms. Bracketing can be introduced to enforce 
  alternative application order for example $xyz$ is implicitly read as 
  $(xy)z$ but an be written as $x(yz)$ to describe that the application of 
  $yz$ should come first. Examples of applications are:
    \begin{figure}[!h]
      \[
        \begin{array}{l}
        xy \\
        xyz \\
        x(yz) \\
        (`lx.x)y
        \end{array}
      \]
    \caption{Examples of valid applications}
    \end{figure}

\subsection{Reduction Rules}

  First we will introduce the substitution notation $M[N/x]$. This denotes 
  the term M with all occurrences of x replaced by N. The substitution 
  notation is defined inductively as:
   
    \begin{figure}[!h]
    \definition{ 
      \textsc{(Substitution notation for \lam-terms)}
      \[
      \begin{array}{rclr}
      x[y/x] & \rightarrow & y \\
      z[y/x] & \rightarrow & z & (z \neq x) \\
      (`lz.M)[y/x] & \rightarrow & `lz.(M[y/x]) \\
      (M N)[y/x] & \rightarrow & M[y/x] N[y/x]
      \end{array}
      \]
    }
    \end{figure}

\subsubsection{\bta-reduction}
  The main derivation rule of the \lam-calculus is \bta-reduction. If term
  $M$ \bta-reduces to term $N$, we write $M \rightarrow_{`b} N$ although 
  the \bta\ subscript can be omitted if it is clear from context. \bta-reduction
  is defined for the application of two terms:
  \begin{figure}[!h]\label{def:beta-reduction}
  \definition{ 
    \textsc{($`b$-reduction for \lam-calculus)}
    \[
    \begin{array}{rcl}
    (`lx.M) N & \rightarrow_{`b} & M[N/x]
    \end{array}
    \]
  }
  \end{figure}
 
  \lam-variables and \lam-abstactions are \emph{values}: they do not reduce
  to other terms. If a formula is a value, the reduction terminates on that 
  value. Only applications reduce to other terms. This means that an 
  application is a reducible expression or a \emph{redex}. Reducing a 
  redex models the computing of a function. 

% TODO: define free variables and bound variables
\subsubsection{$\alpha$-reduction}
  
  The \lam-calculus defines a reduction rule for renaming variables.
  Variable names are arbitrary and chosen just to denote identity:
  all occurrences of $x$ are the same. This can become a problem
  in the following case:
  
  \[
    (`ly.`lx.xy)(`lx.x)
  \]
 
  After the application is reduced, we have the term:
  
  \[
    `lx.x(`lx.x) 
  \]
  
  In this case, it is ambiguous which \lam-abstraction the right-most $x$
  is bound by. When this term is applied to another, will the substitution
  occur to all occurrences of $x$? From the initial term, it is clear that
  this would be incorrect. The \lam-calculus introduces $`a$-reduction to
  solve this:
  
  \begin{figure}[!h]\label{def:alpha-reduction}
  \definition{ 
    \textsc{($`a$-reduction for \lam-calculus)}
  \[
    \begin{array}{rl}
    `lx.M \rightarrow_{`a} `ly.M[y/x] & (y \notin fv(M))
    \end{array}
  \]
  }
  \end{figure}
 
  This means we can rename the lead variable of an abstraction $M$ on the 
  conditions that: 
  \begin{enumerate}
    \item All variables bound by that abstraction are renamed the same 
    \item The variable it is changed to is not currently in use in $M$ 
  \end{enumerate}
  
  
  %TODO: add alpha reduction(?)
  % \subsection{Reduction Strategies}

\section{Logic and Types}

  There are many formal systems for describing logic.
  These systems attempt to describe the relationship between logical statements.
  Like all formal systems, 
  they allow us to derive new logical statements by following transformation rules.

  \subsection{Implicative Intuitionistic Logic}
  \emph{Implicative Intuitionistic Logic} (IIL) is a subset of Gerhard Gentzen's system of natural deduction.
  It is restricted to only the $\rightarrow \mathcal{I}$ and $\rightarrow \mathcal{E}$ rules.
  Following Brouwer, it eschews the law of excluded middle.
  \footnote{The law of excluded middle says $\vdash P \lor \neg P$}
  
  In natural deduction, the $\to$ symbol represents implication.
  To say $A \to B$ is to say that whenever $A$ is true,
  we know $B$ is true.
  The statement should be read ``$A$ implies $B$''.
  
  Logical \emph{inference rules} describe that if some statement $A$ is true,
  we can take for granted that some other statement $B$ is true.
  The notation for this is:
  
  \[
    \Inf{A}{B} 
  \]
  
  Using this notation, we can now describe the inference rules for IIL:
  \[
  \begin{array}{rl@{\quad\quad}rl}
    ( \to \mathcal{E} ) &
    \Inf { A \to B \hspace{0.5cm} A }
        {B}
    &
    ( \to \mathcal{I} ) &
    \Inf { \infer*{B}{[A]} }
        { A \to B}
  \end{array}
  \]
  
  The $\to\mathcal{E}$ rule says that given the statement $A \to B$ and the statement $A$,
  we can conclude $B$.
  For instance, 
  consider $A$ = ``It is raining''
  and $B$ = ``It is wet outside''.
  The statement $A \to B$ becomes ``If (it is raining) then (it is wet outside)''.
jk  If we know that ``If (it is raining) then (it is wet outside)'' and we are told ``It is raining'',
  we can take for granted ``It is wet outside''.
  
  The $\to\mathcal{I}$ rule says that if we assume $A$ and from that assumption we deduce $B$,
  we can conclude that $A \to B$.
  Let us assume that Alan Turing did not see Alonzo Church's work.
  From that assumption, we deduce that Alan Turing could not have stolen Alonzo Church's work.
  Therefore we can conclude that ``If Alan Turing did not see Alonzo Church's work, he could not have stolen it''. 
  
  \subsection{Classical Logic}
  
  %TODO description of classical logic
  
  First, we introduce the notation for a \emph{sequent}:
  
    \[
      `G \vdash T
    \]
    
  The $`G$ is a sequence of statements called the \emph{antecedent}.
  The right-hand side of the $\vdash$ is also a sequence of statements called the \emph{succedent}.
  The whole sequent denotes that the conjunction of the statements in the antecedent imply the disjunction of the statements in the succedent.
  That is to say, all the statements on the left taken together imply that at least one of the statements on the right is correct.
  
  \[
    \begin{array}{c}
    A_1,A_2,\dots,A_n \vdash S_1,S_2,\dots,S_m \\
      \text{denotes} \\
    A_1 \wedge A_2 \wedge \ldots \wedge A_n \to S_1 \lor S_2 \lor \dots \lor S_m \\
    \end{array}
  \]
 

  \subsection{Type Assignment}
  
  Type assignment introduces additional grammar and restrictions on
  the reduction rules of a system. These extensions prevent logically
  inconsistent terms from being constructed. A type assignment has the
  form:
  
  \[
    M : `a 
  \]
  
  which states that term $M$ has the type $`a$. Like variables, type 
  variables are abstract: they do not describe anything more about a 
  type than its identity. That is to say $x: A$ and $y : A$ have the
  same type but we cannot say any more about what that type is.
 
  A type is either some uppercase Latin letter or it is two valid types
  connected by a $\rightarrow$. This is described by the following
  BNF grammar:
  
  \definition{
    \textsc{(Grammar for constructing types)} 
    \item Type variables are represented by the lower-case greek alphabet $`a,`b,`g,...$
    \[
      A,B ::= `v \mid A \rightarrow B 
    \]
  }
  
  \subsection{Typed \lam-Calculus}
  Typed systems have rules for assigning types for terms.
  Type assignment rules for the \emph{typed} \lam-calculus are:

  \[
    \begin{array}{lcrc}
    { (\to\mathcal{E}}) \hspace{4pt} \Inf{
      `G \vdash M : A \to B \hspace{20pt} N:A 
    }{
      `G \vdash M N : B 
    }
    \\
    \\
    { (\to\mathcal{I}}) \hspace{4pt} \Inf{
      `G,x:A \vdash M : B
    }{
      `G \vdash `lx.M : A \to B 
    }
    \end{array}
  \]

  The first rule states that the application of a term of type $A \to B$ to
  a term of type $A$ has type $B$.
  The $\to\mathcal{I}$ rule says a term of type $B$ in a context where $x:A$
  is the same as an abstraction of type $A \to B$ in a context without $x:A$. 
  These rules add restrictions on what constitutes a well-formed term.
  
  \begin{example}[Type-assignment restricts set of valid terms]
  \[  
  \begin{array}{lr}
    (`lx.xx)(`lx.xx) \\
    xx : B \\
    x : A \to B \\
    x : A
  \end{array}
  \]
  
  The variable $x$ is applied to a term so it must have type $A \to B$.
  The term it is applied to must have type $A$.
  However $x$ is applied to itself so it must have type $A$ and $A \to B$.
  This means the term $`lx.xx$ is untypable:
  it is disallowed by the rules of the typed \lam-calculus.
  \end{example}
 
  \subsection{Curry-Howard Isomophism}
  
  Curry-Howard isomorphism states that their is an isomorphism between
  the typed of a term and a logical proposition. The term itself is
  the proof of the proposition.
  
  Looking again at the rules of the typed \lam-calculus and IIL,
  there is a clear correspondence:
  
  \[
    \begin{tabular}{|c|c|}
      \hline
      \lam-calculus & IIL 
    \\ \hline 
        \infer{
          `G \vdash M N : B 
        }{
          `G \vdash M : A \to B \hspace{20pt} N:A 
        }
      &
      \infer{B}
            { A \to B \hspace{0.5cm} A }
    \\ \hline
      \infer{
        `G,x:A \vdash M : B
      }{
        `G \vdash `lx.M : A \to B 
      }
    &
      \infer { \infer*{B}{[A]} }
        { A \to B}
   \\ \hline 
    \end{tabular}
  \]
  
  % TODO: add more - examples and demonstration

\section{Haskell}
  % TODO: describe purity and laziness
  % TODO: add section on exceptions
  
  \subsection{Data Types}
  New data types can be introduced into Haskell in 3 distinct ways. First,
  using the \mono{data} keyword:
  
  \Verbatimcode
    data Animal a = Dog a
      | Cat
  \end{Verbatim}
  
  The \mono{data} keyword begins the definition of a new data type. The
  word immediately following determines the type constructor for the new
  type. Following this is a type parameter for the type constructor. There
  can be any number of type parameters, including zero. The right-hand
  side of the \mono{=} introduces a \mono{|}-separated list of data 
  constructors.
  
  \Verbatimcode
    > let hector = Cat
    > :t hector
    hector :: Animal a
    
    > let topaz = Dog "foo"
    > :t topaz
    topaz :: Animal String
  \end{Verbatim}
  
  The type parameter is constrained by the type of the value the data
  constructor was initialized with. In the example above, calling the
  \mono{Dog} data constructor with a string makes the type \mono{Animal
  String} rather than the more general \mono{Animal a}.
  
  The second method for introducing new data types is the \mono{newtype}
  keyword. The key difference between \mono{data} and \mono{newtype} is
  that \mono{newtype} can only have one data constructor. Informally,
  this implies a kind of isomorphism:

  \Verbatimcode
    newtype Foo a = Foo (a -> Integer)
  \end{Verbatim}
  
  The type constructor can take type parameters which will be constrained
  by the inhabitants of the data constructor. This data type expresses
  an isomorphism between \mono{Foo a} and functions from \mono{a} to
  \mono{Integer}s.
  
  Finally, we can introduce type aliases using the \mono{type} keyword:
  \Verbatimcode
    type Name = String 
  \end{Verbatim}
  
  Again, we introduce a type constructor \mono{Name} but this time we
  name another type, in this case \mono{String}, as its inhabitant. This
  means that the type \mono{Name} is a type alias for \mono{String} and
  will share the same data constructors. 
 
  \subsection{Type Level/Value Level}
  
  Haskell distinguishes between terms on the type level and terms on the value level. 
  This is the same as the separate layer of terms and types in the \lam-calculus.
  Types in Haskell are descriptions of the types of a value.
  They provide restrictions on the construction of invalid terms. 
  For instance if we have a function of type \mono{String -> Integer}, 
  we cannot apply it to a term of type \mono{Boolean}. 
  The type-checker will throw an error before any value-level computation is initiated.
 
  % TODO: better description needed
  The value level is the level on which data is constructed and manipulated.
  The operation \mono{1+1} occurs on the value level. The value level is
  where computation takes place and the type level is where static analysis
  of the program type takes place.
  
  \subsection{Type Classes}
  Haskell adds type classes to the type level. Types can have instances of
  type classes. The most similar concept from Object-Oriented programming
  is \emph{interfaces}.
  
  \Verbatimcode
    class Addable a where
      (add) :: a -> a -> a
  \end{Verbatim}
  
  Type classes are introduced using the \mono{class} keyword. Beneath that
  are the function names and corresponding type-signatures of the functions
  that an instance of a class must implement.
  
  For example, we can create instances of the \mono{Addable} class:
  
  \Verbatimcode
    data Number = One | Two | ThreeOrMore
    
    instance Addable Number where
      add One One = Two 
      add One Two = ThreeOrMore 
      add Two One = ThreeOrMore 
  \end{Verbatim}    
  
  %\subsection{Term Rewriting}
  % explain ($) operator
  % explain monads as generalizations of cps-terms
  % what they encode and why they are useful

% TODO: these explanations of continuations (diff between delim and 
%       undelim) are incorrect 
\section{Continuations}
  
  Compound terms can be decomposed into two seperate parts: a dominant 
  term and a context. The dominant term is the term currently being 
  evaluated. The context is a term with a hole that will be filled with 
  the value the dominant term reduces to.
  
  \begin{figure}[!h]
    \hspace{1cm}Assume that $M \rightarrow_{`b} M^\prime$
    \[
    \begin{array}{lrcl}
    \textit{(Compound term)}&& MN \\
    \textit{(Decompose)}&M && \square N \\
    \textit{(Beta-reduce dominant term)}& M^\prime && \square N \\
    \textit{(Refill hole of context)}&& M^\prime N \\
    \end{array}
    \]
  \caption{Decomposing a term into a dominant term and a context}
  \end{figure}
  
  When $M$ has \bta-reduced to a value -- $M^\prime$ -- then the hole
  of the context $\square N$ is filled to form $M^\prime N$. What the
  dominant term and context are for a given term depends on the
  reduction rules and strategy. The context is what remains to be
  reduced at given moment of reduction. Thus a context is also called
  a \emph{continuation}.
 
  \subsection{Undelimited Continuations} 
 
  For more complex terms, the waiting context will grow as the dominant
  term gets further decomposed:
  
  \begin{figure}[!h]
    \[
    \begin{array}{ll}
      (MM^\prime) M^{\prime\prime} \\
      (MM^\prime) & \square M^{\prime\prime} \\
      M & (\square M^\prime) M^{\prime\prime} \\
    \end{array}
    \]
  \caption{Decomposing a term into multiple contexts}
  \end{figure}

  By amalgamating continuatinos into one big continuation we only have
  two components at point during the reduction: the current dominant term
  and the \emph{current continuation}. 
  
  Assume we have some reduction rules defined for manipulating
  continuations. This model keeps continuations grouped together which
  means these hypothetical reduction rules could manipulate only this
  entire remaining continuation. For this reason, continuations that
  can only be manipulated in their entirety are \textbf{undelimited
  continuations}.

  \subsection{Delimited-Continuations}

  Instead, if we maintain a stack of continuations when
  decomposing complex terms, we can keep continuations separated:
  
  \begin{figure}[!h]
    \[
    \begin{array}{lll}
      (MM^\prime) M^{\prime\prime} \\
      (MM^\prime) & \square M^{\prime\prime} \\
      M & \square M^\prime & \square M^{\prime\prime} \\
    \end{array}
    \]
  \caption{Decomposing a term into multiple contexts}
  \end{figure}

  Here, when a dominant term has been reduced, the reduct is returned
  to its corresponding continuation. This newly joined term then
  becomes the dominant redex. After this new dominant term has been
  reduced, it will be returned to the next waiting continuation, and
  so on. Throughout this process, we maintain each continuation
  separately.
  
  Assume again that we have reduction rules defined for manipulating
  continuations. By keeping continuations separate, this model would 
  allow use to use parts of the stack selectively for instance placing
  delimiters between portions of interest. The increased granularity of 
  control means we can manipulate not just the entire remaining continuation 
  but sections of it. Thus, continuations of this kind are called 
  \textbf{delimited continuations}.

  \subsection{Continuation-Passing Style}
 
  By rewriting \lam-terms, the continuations can be made explicit. All
  terms must be turned into \lam-abstractions of some variable $k$
  where $k$ is the continuation of a term. $k$ is then called on the
  result of the term, triggering the continuation to take control.
  This style of writing \lam-terms is called continuation-passing
  style or CPS.
  
  \definition{
    \textsc{(Translation of standard \lam-terms into CPS)} 
    \[
    \begin{array}{lcl}
      \tr{x}     & = & `lk.kx      \\
      \tr{`lx.M} & = & `lk.k(`lx.\tr{M}) \\
      \tr{M N} & = & `lk.M(`lm.m\tr{N}(`ln.mnk)
    \end{array}
    \]
  }
  
  The term that a CPS program terminates on will be of the form
  $`lk.kM$. In order to extract the value, a \emph{final continuation}
  must be provided. Depending on the context, this could be an identity 
  function $`lx.x$ or a display operation $`lx.\text{display }x$ to
  display the results of the program.

  \begin{figure}[!h]
  \caption{Extracting the final value from a terminated CPS program}
  \[
  \begin{array}{ll}
                & (`lk.kM)(`lx.x) \\
    \rightarrow & (`lx.x)M \\
    \rightarrow & M
  \end{array}
  \]
  \end{figure}
 
  The translation of standard \lam-terms into CPS similarly transforms the 
  \emph{types} of \lam-terms. For example, a term $x : A$ becomes 
  $`lk.kx : (A \to B) \to B$. This type represents a delayed computation:
  a computation that is waiting for a function to continue execution with. 
  In order to resume the computation, the term must be applied to a 
  continuation.
  
  As an example, take the term M where
  
  \[
    M = `lk.kx
  \]

  To access the value $x$ contained in $M$, we have to apply $M$ to a
  continuation function $`lm.N$:
  
  \[
  \begin{array}{l}
      (`lk.kx)(`lm.N) \\
      (`lm.N)x \\
      N[x/m]
  \end{array}
  \]
  
  Within the body of $N$, $m$ is bound to the value contained by $M$.
  So we can think about $M$ as a suspended computation that, when applied
  to a continuation, applies the continuation to $x$. Looking at
  the type $(A \to B) \to B$ again, it is clear that $A$ is the type
  of the term passed to the continuation of a CPS-term:
  
  \[
  \begin{array}{l}
    `lk.kx : (A \to B) \to B \\
    k : (A \to B) \\
    kx : B \\
    x : A \\
  \end{array} 
  \]
  
  % TODO: explain how order-of-evaluation can be enforced using 
  %       cps.
 
  \subsection{Monads}
  
  If we have two suspended computations $M$ and $M^\prime$ and we want to
  run $M$ and then $M^\prime$, we have to apply $M$ to a continuation
  to access its value and then do the same to $M^\prime$:
  
  \[
    M(`lm.M^\prime(`lm^\prime.N))
  \]
  
  This is a common operation so we define a utility operator \mono{>>=}
  that binds the first suspended computation to a continuation which 
  returns another suspended computation\footnote{ \mono{(>>=)} is pronounced 'bind'.}:
 
  \[
    \mono{>>=} : ((A \to B) \to B) \to (A \to ((B \to C) \to C)) \to ((B \to C) \to C)
  \]
 
  The type $(A \to B) \to B$ that represents a suspended computation returning
  a value of type $A$ to its continuation we will call an $A$-computation or
  $Comp\ A$. We can rewrite the type signature of \mono{>>=}:
  
  \[
    \mono{>>=} : Comp\ A \to (A \to Comp\ B) \to Comp\ B
  \]
  
  We define another operator, $return$, that takes a value and returns a 
  suspended computation that returns that value:
  
  \[
    return : A \to Comp\ A 
  \]
 
  The type constructor $Comp\ A$, together with the two utility functions
  \mono{>>=} and $return$, make up Haskell's Monad type class:
  
  \Verbatimcode
    class Monad M where
      (>>=) :: M a -> (a -> M b) -> M b
      return :: a -> M a
  \end{Verbatim}
  
  The Monad type class generalizes CPS terms: they represent suspended
  computations that can be composed using \mono{>>=}. Just like CPS terms,
  a Monad type \mono{M a} tells us that we have a term that will pass
  values of type \mono{a} to the continuation it is bound to using
  \mono{>>=}.
  
  % TODO: finish explanation of linking suspended-computations and CPS
  %       to monads

\section{\lmu-Calculus}
  % TODO: intuitively, that mu-terms are expressed easily by continuation 
  %       passing style gives us some idea that they are about control flow
  %       and that they will be easily expressed by monads
  \subsection{Syntax}
  \begin{figure}[!h]
  \definition{ 
    \textsc{(Grammar for \lmu-calculus)}
    \item $`l$-variables are denoted by $x, y,\dots$ and $`m$-variables are denoted by $`a, `b,\dots$ \\
    \[
    \begin{array}{lrcl}
    
    \text{(Unnamed term)} & M,N & ::= & x\ |\ `lx.M\ |\ M\ N\ |\ `m`a.C \\
    \text{(Named term)} & C & ::= & [`a]M
    \end{array}
    \]
  }
  \end{figure}
  
  Just as \lam\ introduces a new \lam-abstraction, \lmu\ introduces a new 
  \lmu-abstraction. The body of a \lmu-abstraction must be a named term. 
  A named term consists of a name of the form $[`a]$ followed by an unnamed 
  term. 

  \subsection{Reduction Rules}
  \begin{figure}[!h]
  \definition{ 
    \textsc{(Reduction rules for \lmu-calculus)}
    \[
    \begin{array}{rcl}
    x & \rightarrow & x \\
    `lx.M & \rightarrow & `lx.M \\
    `m`a.[`b]M & \rightarrow & `m`a.[`b]M \\
    (`lx.M) N & \rightarrow & M[N/x] \\
    (`m`a.[`b]M) N & \rightarrow & (`m`a.[`b]M[[`g]M^\prime N/[`a]M^\prime]) \\
    \end{array}
    \]
  }
  \end{figure}

  The terse reduction rule at the end simple states that the application
  of a \lmu-abstraction $`m`a.M$ to a term $N$ applies all the sub-terms 
  of $M$ labelled $[`a]$ to $N$ and relabels them with a fresh $`m$ 
  variable.
  
  \subsection{Computational Significance}

  % TODO: use cps translation to explain mu

  The additional \lmu\ reduction rules model context manipulation. 
  \lmu-variables map to contexts. When an unnamed term is labelled with a 
  \lmu-variable, it is evaluated in that context. For instance the named 
  term $['a]M$ has the effect of evaluating $M$ in the context pointed to 
  by $`a$.
  
  To make this more concrete, consider the compound term $(`m`a.[`b]M)\ N$. 
  First we decompose the term into a dominant term $(`m`a.[`b]M)$ and a 
  context $\square N$. Informally, we can imagine that the \lmu-variable 
  $`a$ now maps to this context $\{`a \Rightarrow \square N\}$:
  
  % TODO: reform to get rid of context mapping:
  %   keep mu abstraction in dominant and consult context for
  %   what to apply named-terms to
  \begin{example}[]
    \[
    \begin{array}{lc}
    \textbf{Dominant} & \textbf{Context} \\
    (`m`a.[`b]M) N \\
    `m`a.[`b]M & \square N \\
    \end{array}
    \]
  \end{example}

  All subterms of $M$ labelled $`a$ will now be evaluated in the context 
  $\square N$ and the context will destroyed. For example, let us replace 
  $M$ with \mbox{$`m\circ.[`a](`ls.fs)$}:
  
  \begin{example}
    \[
    \begin{array}{lcr}
    \textbf{Dominant} & \textbf{Context} \\
    `m`a.[`b]`m\circ.[`a](`ls.fs)    & \square N \\
    `m`a.[`a](`ls.fs)    & \square N \\
    `m`g.[`g](`ls.fs)N   & & (`g\ \text{fresh})  \\
    \end{array}
    \]
  \end{example}

  % TODO: explain applicative contexts
  After applying the term $[`a](`ls.fs)$ to $N$, the context $\square N$
  is consumed and every occurrence of $`a$ is replaced with a fresh 
  variable -- in this case a $`g$ -- to clarify that the new 
  \lmu-abstraction points to a new context. This means that 
  \lmu-abstractions will pass all of the applicative contexts to the
  named subterms:
  
  \begin{example}
    \[
    \begin{array}{lcr}
    \textbf{Dominant} & \textbf{Context} \\
    (`m`a.[`a](`ls.`lt.st)) M N \\
    (`m`a.[`a](`ls.`lt.st))M & \square N \\
    `m`a.[`a](`ls.`lt.st) & \square M:\square N \\
    `m`g.[`g](`ls.`lt.st)M & \square N & (`g\ \text{fresh}) \\
    `m`d.[`d](`ls.`lt.st)MN & \square N & (`d\ \text{fresh}) \\
    \end{array}
    \]
  \end{example}
  
  % \subsection{Reduction Strategies}
  \subsection{Isomorphism \& Computational Interpretation}

\section{Delimited-Continuation Calculus}

  % TODO: explain motivation behind DCC: \lmu requires
  %       delimited continuations and DCC formalizes delimited
  %       continuations in haskell so useful guide for implementing
  %       exceptions in haskell

  Simon Peyton-Jones \textit{et al.}\ extended the \lam-calculus with additional operators in order create a framework for implementing delimited continuations \cite{JonesDS07}. This calculus will be referred to as the delimited-continuation calculus or DCC. Many calculi have been devised with control mechanisms. Like the \lmu-calculus, these control mechanisms are all specific instances of delimited and undelimited continuations. DCC provides a set of operations that are capable of expressing many of these other common control mechanisms.

  The grammar of DCC is an extension of the standard \lam-calculus:

  \subsection{Syntax}
  \begin{figure}[!h]
  \definition{ 
    \textsc{(Grammar for DCC)}
    \[
    \begin{array}{lrcl}
    \textrm{(Variables)} & x, y, \dots \\
    \textrm{(Expressions)} & e & ::= & x\ |\ `lx.e\ |\ e\ e^\prime \\
                           &   &  |  &  newPrompt\ |\ pushPrompt\ e\ e \\
                           &   &  |  &  withSubCont\ e\ e\ |\ pushSubCont\ e\ e
    \end{array}
    \]
  }
  \end{figure}

  \subsection{Reduction Rules}
  The operational semantics can be understood through an abstract machine that transforms tuple of the form $\langle e,\ D,\ E\, q \rangle$:

  \begin{figure}[!h]
  \relscale{0.9}
  \definition{ 
    \textsc{(Operational semantics for DCC)}
    \[
    \begin{array}{lrcl}
      \langle e\ e^\prime, D, E, q \rangle &\Rightarrow &\langle e, D[\square\ e^\prime], E, q \rangle &\text{e non-value} \\
      \langle v\ e, D, E, q \rangle &\Rightarrow &\langle e, D[v\ \square], E, q \rangle &\text{e non-value} \\
      \langle pushPrompt\ e\ e^\prime, D, E, q \rangle &\Rightarrow &\langle e, D[pushPrompt\ \square\ e^\prime], E, q \rangle &\text{e non-value} \\
      \langle withSubCont\ e\ e^\prime, D, E, q \rangle &\Rightarrow &\langle e, D[withSubCont\ \square\ e^\prime], E, q \rangle &\text{e non-value} \\
      \langle withSubCont\ p\ e, D, E, q \rangle &\Rightarrow &\langle e, D[withSubCont\ p\ \square], E, q \rangle &\text{e non-value} \\
      \langle pushSubCont\ e\ e^\prime, D, E, q \rangle &\Rightarrow &\langle e, D[pushSubCont\ \square\ e^\prime], E, q \rangle &\text{e non-value} \\
    \\
      \langle (`lx.e)\ v, D, E, q \rangle &\Rightarrow &\langle e[v/x], D, E, q \rangle \\
      \langle newPrompt, D, E, q \rangle &\Rightarrow &\langle q, D, E, q+1 \rangle \\
      \langle pushPrompt\ p\ e, D, E, q \rangle &\Rightarrow &\langle e, \square, p : D : E, q \rangle \\
      \langle withSubCont \ p\ v, D, E, q \rangle &\Rightarrow &\langle v (D : E\textsmaller[1]{\overset{p}{\uparrow}}, \square, E\textsmaller[1]{\overset{p}{\downarrow}}, q \rangle \\
      \langle pushSubCont E^\prime\ e, D, E, q \rangle &\Rightarrow &\langle e, \square, E^\prime +{+} (D : E), q \rangle \\
    \\
      \langle v, D, E, q \rangle &\Rightarrow &\langle D[v], \square, E, q \rangle \\
      \langle v, \square, p : E, q \rangle &\Rightarrow &\langle v, \square, E, q \rangle \\
      \langle v, \square, D : E, q \rangle &\Rightarrow &\langle v, D, E, q \rangle
    \end{array}
    \]
  }
  \end{figure}
  
  \subsection{Significance}

  % TODO: ensure prompts and continuation stack has been explained before reaching this point
  The additional terms behave as follows:
  \begin{itemize}
  \item \op{newPrompt} returns a new and distinct prompt.
  \item \op{pushPrompt}'s first argument is a prompt which is pushed onto the continuation stack before evaluating its second argument. 
  \item \op{withSubCont} captures the subcontinuation from the most recent occurrence of the first argument (a prompt) on the excution stack to the current point of execution. Aborts this continuation and applies the second argument (a \lam-abstraction) to the captured continuation.
  \item \op{pushSubCont} pushes the current continuation and then its first argument (a subcontinuation) onto the continuation stack before evaluating its second argument.
  \end{itemize}
  
\section{\ltry-Calculus}

Steffen van Bakel extended the \lam-calculus with operators for modelling exceptions.
Unlike previous systems, the \ltry-calculus uses named exceptions.

\subsection{Exceptions}

Exceptions in programming languages are indications that control flow cannot continue.
For example, if you attempt to open a non-existent file,
the operation might throw an exception.
If an exception occurs without being caught, a program will exit with an error.
Exceptions can be caught and attempts at recovery can be made by exception handlers.

The common syntax for introducing exception handlers is in try-catch blocks.
An exception that occurs in a try-catch block will be handled by a corresponding handler.
For example, see the Javascript syntax for this:

\begin{Verbatim}
  try {
    /* possibly throw exception */
  } catch (e) {
    /* recover thrown exception */ 
  }
\end{Verbatim}

The \mono{catch (e) \{ ... \}} introduces a single exception handler.
This exception handler will be called if an exception is thrown inside the try block.
If we want to introduce multiple exception handlers,
we need a mechanism for deciding which handler will be called.
Java solves this by registering different exception handlers based on their type:

\begin{Verbatim}
  try {
    /* possibly throw exception */
  } catch (IOException e) {
    /* recover from IOException */  
  } catch (FileNotFoundException e) {
    /* recover from FileNotFoundException */ 
  }
\end{Verbatim}

Here, which handler is called depends on the type of the exception thrown.

\subsection{Syntax}

The grammar of the \ltry-calculus is as follows:
\definition{
\textsc{(Grammar of the \ltry-calculus)}
\[
  \begin{array}{rclr}
    C &::=& \catch{ n$_1$($x$) = $M_1$ }; \dots; \catch{n$_i$($x$) = $M_i$} & (i \geq 1) \\
    M,N &::=& x\ |\ `lx.M\ |\ MN\ |\ \try M;\ C\ |\ \throw{n($M$)}
  \end{array}
\]
}

The grammar for $C$ describes a catch block as series of one or more catch statements. 
For convenience, we will use the notation 

\[
  \mult{\catch{n$_i$($x$) = $M_i$}}{i > 1}
\]

to describe a catch block with more than one catch statement. 

The \ltry-calculus adds three new syntactic constructs:
\begin{itemize}
\item \textbf{throw n($M$)} denotes the throwing of an exception with name $n$ passing it the value $M$.
\item \textbf{catch n($x$) = $M$} registers the exception handler $M$ to the name $n$ with parameter $x$.
\item \textbf{try $M$; $C$} attempts to run term $M$ in an environment with the exception handlers in catch block $C$ registered.
\end{itemize}

\subsection{Reduction Rules}
In conjunction with the additional syntactic constructions,
the \ltry-calculus introduces some reduction rules:

\definition{
\textsc{(\ltry\ reduction rules)}
\[
\begin{array}{rlcl}
  \text{($`b$):}      & (`lx.M)N &\to& M[N/x] \\
  \text{(throw):}     & (\throw{n($M$)})N &\to& \throw{n($M$)} \\
  \text{(try-throw):} & \try \throw{n$_l$($N$)};\ \mcatch &\to& M_l[N/x] \\
  \text{(try-value):} & \try V;\ \mcatch &\to& V \\
\end{array}
\]
}

The $`b$ reduction rule is familiar from the \lam-calculus. 
A \textbf{throw} term applied to any term discards the second term.
A \textbf{try} term that contains a throw reduces to the handler that corresponds to the name of the exception thrown with all occurrences of the parameter replaced by the value thrown. 
For instance a $\throw{n(N)}$ inside a \textbf{try} will reduce to $M[N/x]$ if there is a $\catch{n($x$) = $M$}$ in the catch block.
A \textbf{try} term that contains a value reduces to just that value.

\subsection{Significance}
The occurrence of an exception aborts the current computation.
\ltry\ models this by discarding terms that a \textbf{throw} is applied to.
The \textbf{try-catch} statements mirror the syntax of try-catch statements in programming languages in the C-syntax family.

