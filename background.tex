% TODO: introduce notation and concept of fv/bv and fn/bn

\chapter{Background}\label{chapter:background}

This chapter explores what \emph{formal systems} are and what they are useful for. It looks at a number of related formal systems and their relation to computation. It outlines the context ontop of which the rest of this project is built.

% TODO: explain that ideas will become clearer as they are
%       used and we see how they interact with other concepts
%       introduced later. it's ok to leave concepts not entirely
%       developed

\section{Formal Systems}

Formal systems are a set of rules for writing and manipulating
formulae. Formulae are constructed from a set of characters called the
\emph{alphabet} by following a some formula-construction rules called
the \emph{grammar}. The only formulae considered \emph{well-formed} in
a system are those constructed according to the grammar of a system.
Formal systems are used to model domains of knowledge to help better
and more formally understand those domains.

\subsection{Syntax and Grammars}

The grammar of a formal system describes the system's syntax. Grammars are 
rules for constructing formulae that are well-formed. Formulae
produced according to the grammar of a system are well-formed according to 
the syntax of that system.

We defined grammars using Backus-Naur Form or BNF:

\[
  M ::= t\ \mid\ f
\]

This grammar describes that syntactically-valid constructs are either
the letter $t$ or the letter $f$. Grammars can be recursive
which allows much more expressive construction rules:

\begin{figure}[!h]\label{fig:tf-grammar}
\[
  \begin{array}{lclr}
    M,N&::=&t&\textcolor{pale-gray}{(1)} \\
      &|&f&\textcolor{pale-gray}{(2)} \\
      &|&M\ a\ N&\textcolor{pale-gray}{(3)} \\
      &|&M\ o\ N&\textcolor{pale-gray}{(4)}
  \end{array}
\]
\caption{Grammar for producing the letters $t$ or $f$ connected by the letters $a$ or $o$}
\end{figure}

This grammar describes formulae containing the any number of occurrences of
the letters $t$ or $f$ separated by either an $a$ or an $o$.

\[
\begin{array}{lr}
  \by{t}{1} \\
  \by{t\ o\ f}{2 \& 4} \\
  \by{t\ o\ f\ a\ t}{1 \& 3}
\end{array}
\]

\subsection{Derivation Rules}
Whereas a grammar describes the rules for producing well-formed formulae,
the derivation rules describe rules for transforming formulae of a
particular form into a new formula. Using the grammar from Figure 
\ref{fig:tf-grammar}, we add derivation rules:

\[
\begin{array}{lcl}
  t\ a\ M &\to& M \\ 
  M\ a\ t &\to& M \\
  f\ a\ M &\to& f \\
  M\ a\ f &\to& f \\
  \\
  M\ o\ t &\to& t \\ 
  t\ o\ M &\to& t \\
  f\ o\ f &\to& f \\
\end{array}
\]

These rules describe that if a formula matches the pattern on the 
left-hand side, where $M$ represents a well-formed formula, it can be
replaced by the formula on the right-hand side.

\subsection{Domain Modelling}

The syntax and derivation rules of a formal system are defined to model 
some domain. This isomorphism between the domain and the formal system
means we can attempt to discover truths about the domain through studying
the formal system.

For example, take the $tf$-system described above. Without
understanding the domain, we are able to manipulate formulae of the
system to create new formulae. The $tf$-system is isomorphic to
Boolean algebra:

\[
\begin{array}{cc}
\text{$tf$-system} & \text{Boolean algebra} \\
t & 1 \\
f & 0 \\
a & \wedge \\
o & \vee 
\end{array}
\]

Using formal systems allows us to understand the domains they model from
different perspectives and thereby learn novel truths about them.

\subsection{Derivation Strategies}
% TODO: explain that when you can make more than one decision,
%       you can have a strategy for which decision you take

When applying derivation rules to compound terms, we can imagine the 
compound term being decomposed into simpler terms until one of the 
derivation rules applies. When we decompose a term, we separate it into a
dominant term and a context:

\begin{figure}[!h]\label{fig:decomposing}
\[
\begin{array}{lcr}
  & t\ o\ f\ a\ t \\
  t\ o\ f\ && \square\ a\ t 
\end{array}
\]
\end{figure}

The left-hand side is the dominant term and the right-hand side is the context. 
The $\square$ in the context denotes a hole that needs to be filled to create a full term.

Once we have a term that we can apply a derivation rule to, 
we apply the derivation rule and recombine the context with the resulting term:

\[
\begin{array}{rlcr}
      & t\ o\ f\ && \square\ a\ t  \\
  \to & t && \square\ a\ t  \\
  \textit{recombine} & t\ a\ t 
\end{array}
\]

The term $t\ o\ f\ a\ t$ can be decomposed in two ways:

\[
\begin{array}{rlr}
  1. & t\ o\ f\ & \square\ a\ t  \\
  2. & f\ a\ t & t\ o\ \square  \\
\end{array}
\]

When we have more than one way derivation rules can be applied to a term,
we can use \emph{derivation strategies} to determine which rule we apply.
The derivation strategy we use decides how our compound terms are decomposed.
In our $tf$-system, there are two obvious derivation strategies:
either apply derivations starting from the left or starting from the right.

\[
\begin{array}{rcl}
  
  \textbf{(left)} & t\ o\ f\ a\ t \\
  \to                & t\ o\ f & \square\ a\ t \\
  \to                & t       & \square\ a\ t \\
  \textit{recombine} & t\ a\ t & \\
  \\
  \textbf{(right)}   & t\ o\ f\ a\ t \\
  \to                & f\ a\ t & t\ o\ \square \\
  \to                & f       & t\ o\ \square \\
  \textit{recombine} & t\ o\ f & \\
  
\end{array}
\]
Whereas derivation rules are defined in the system, 
derivation strategies are methods of choosing which derivation rule to apply when given a choice. 

\section{\lam-Calculus}

In response to Hilbert's \emph{Entscheidungsproblem}, Alonzo Church
defined the \lam-calculus. It is a formal system capable of expressing
the set of effectively-computible algorithms. Ontop of this, he built
his proof that not all algorithms are decidable. Shortly after, Godel
and Turing created their own models of effective computibility.
\footnote{General recursive functions and Turing machines, respectively} 
These models were later proved to all be equivalent.

\subsection{Syntax}
  
  \lam-variables are represented by $x,y,z,etc$. Variables denote an
  arbitrary value: they do not describe what the value is but that any
  two occurrences of the same variable represent the same value. The
  grammar for constructing well-formed \lam-terms is:

  \begin{figure}[!h]
  \definition{ 
    \textsc{(Grammar for untyped \lam-calculus)}
    \item $`l$-variables are denoted by $x, y,\dots$ \\
    \[
    \begin{array}{rcll}
    M,N & ::= & x         & \text{(Variable)} \\
        & \mid\ & `lx.M\  & \text{(Abstraction)} \\ 
        & \mid\ & M\ N    & \text{(Application)} \\ 
    \end{array}
    \]
  }
  \end{figure}

  \lam-abstractions are represented by $`lx.M$ where $x$ is a
  parameter and $M$ is the body of the abstraction. The same idea is
  expressed by more conventional notation as a mathematical function
  $f(x) = M$. The $`l$ annotates the beginning of an abstraction and
  the $.$ separates the parameter from the body of the abstraction.
  This grammar is recursive meaning the body of an abstraction is just
  another term constructed according to the grammar. Some examples of
  abstractions are:
  
  \begin{figure}[!h]
    \[
      \begin{array}{l}
      `lx.x \\
      `lx.xy \\
      `lx.(`ly.xy)
      \end{array}
    \]
  \caption{Examples of valid \lam-abstractions}
  \end{figure}
  
  Applications are represented by any two terms, constructed according to
  the grammar, placed alongside one another. Application gives highest
  precedence to the left-most terms. Bracketing can be introduced to enforce 
  alternative application order for example $xyz$ is implicitly read as 
  $(xy)z$ but can be written as $x(yz)$ to describe that the application of 
  $yz$ should come first. Examples of applications are:
    \begin{figure}[!h]
      \[
        \begin{array}{l}
        xy \\
        xyz \\
        x(yz) \\
        (`lx.x)y
        \end{array}
      \]
    \caption{Examples of valid applications}
    \end{figure}

\subsection{Reduction Rules}

  First we will introduce the substitution notation $M[N/x]$. This denotes 
  the term M with all occurrences of x replaced by N. The substitution 
  notation is defined inductively as:
   
    \begin{figure}[!h]
    \definition{ 
      \textsc{(Substitution notation for \lam-terms)}
      \[
      \begin{array}{rclr}
      x[y/x] & \rightarrow & y \\
      z[y/x] & \rightarrow & z & (z \neq x) \\
      (`lz.M)[y/x] & \rightarrow & `lz.(M[y/x]) \\
      (M N)[y/x] & \rightarrow & M[y/x] N[y/x]
      \end{array}
      \]
    }
    \end{figure}

\subsubsection{\bta-reduction}
  The main derivation rule of the \lam-calculus is \bta-reduction. If term
  $M$ \bta-reduces to term $N$, we write $M \rightarrow_{`b} N$ although 
  the \bta\ subscript can be omitted if it is clear from context. \bta-reduction
  is defined for the application of two terms:
  \begin{figure}[!h]\label{def:beta-reduction}
  \definition{ 
    \textsc{($`b$-reduction for \lam-calculus)}
    \[
    \begin{array}{rcl}
    (`lx.M) N & \rightarrow_{`b} & M[N/x]
    \end{array}
    \]
  }
  \end{figure}
 
  \lam-variables and \lam-abstactions are \emph{values}: they do not reduce
  to other terms. If a formula is a value, the reduction terminates on that 
  value. Only applications reduce to other terms. This means that an 
  application is a reducible expression or a \emph{redex}. Reducing a 
  redex models the computing of a function. 

% TODO: define free variables and bound variables
\subsubsection{$`a$-reduction}
 
  The variables in a \lam-term are either \emph{bound} or \emph{free}. 
  The bound variables of a term are those introduced by \lam-abstractions before their use.
 
  \definition{
  \textsc{Bound variables $bv$ of \lam-terms}
  \cite{Bakel15}
  \[
  \begin{array}{rcl}
    bv(x)     & = & \varnothing \\
    bv(`lx.M) & = & bv(M) \cup \{x\} \\
    bv(MN) & = & bv(M) \cup bv(N) \\
  \end{array} 
  \]
  }
  \definition{
  \textsc{Free variables $fv$ of \lam-terms}
  \\
  The free variables of a term $M$ are variables that occur in $M$ that are not bound.
  \\
  }
 
  % TODO: explain this in terms of bound and free variables
  The \lam-calculus defines a reduction rule for renaming variables.
  Variable names are arbitrary and chosen just to denote identity:
  all occurrences of $x$ are the same. This can become a problem
  in the following case:
  
  \[
    (`ly.`lx.xy)(`lx.x)
  \]
 
  After the application is reduced, we have the term:
  
  \[
    `lx.x(`lx.x) 
  \]
  
  In this case, it is ambiguous which \lam-abstraction the right-most $x$
  is bound by. When this term is applied to another, will the substitution
  occur to all occurrences of $x$? From the initial term, it is clear that
  this would be incorrect. The \lam-calculus introduces $`a$-reduction to
  solve this:
  
  \begin{figure}[!h]\label{def:alpha-reduction}
  \definition{ 
    \textsc{($`a$-reduction for \lam-calculus)}
  \[
    \begin{array}{rl}
    `lx.M \rightarrow_{`a} `ly.M[y/x] & (y \notin fv(M))
    \end{array}
  \]
  }
  \end{figure}
 
  This means we can rename the lead variable of an abstraction $M$ on the 
  conditions that: 
  \begin{enumerate}
    \item All variables bound by that abstraction are renamed the same 
    \item The variable it is changed to is not currently in use in $M$ 
  \end{enumerate}
  
  
  \subsection{Normal Forms}
 
  A term is a in normal form for a reduction strategy if, 
  following that reduction strategy,
  no more reductions can take place.
  For instance, a term that can no longer be \bta-reduced is in \bta-normal form.
  We can define \bta-normal form on \lam-terms with the following BNF grammar:
  We use $\to_{`b}^{nf}$ to denote a term in \bta-normal form.
  Again, we can omit the \bta\ subscript if it is clear from context.

\section{Logic and Types}

  There are many formal systems for describing logic.
  These systems attempt to describe the relationship between logical statements.
  Like all formal systems, 
  they allow us to derive new logical statements by following transformation rules.

  Logic relates statements together with logical connectives.
  Under an interpretation, a statement is either true or false.
  Depending on the truth value of the component statements, a compound statement is either true or false.
  As far as we are concerned, the logical connectives consist of $\land, \lor, \to$, and $\neg$.
  \begin{itemize}
    \item $M \land N$ (read: `and') is true only when both $M$ and $N$ are true.
    \item $M \lor N$ (read: `or') is true only if either $M$ or $N$ are true. 
    \item $M \to N$ (read: `implies') is false when $M$ is true but $N$ is false.
    \item $\neg M$ (read: `not') is true only when $M$ is false.
  \end{itemize}
  The symbol $\to$ represents implication: whenever $M$ is true, $N$ is true.
  
  \subsection{Natural Deduction}
  
  In natural deduction, there are \emph{introduction} and \emph{elimination} rules for each logical connective.
  These are presented as \emph{inference rules}.
  Logical inference rules describe that if some statement $A$ is true,
  we can take for granted that some other statement $B$ is true.
  This is denoted by:
  
  \[
    \Inf{A}{B} 
  \]
  
  Using this notation, we can now describe the inference rules for $\to$.
  For our purposes, this is the only logical connective we are interested in.
  The reason for this will become clear.
  
  \definition{
  \textsc{$\to$ introduction and elimination rules} 
  \[
  \begin{array}{rl@{\quad\quad}rl}
    ( \to \mathcal{E} ) &
    \Inf { A \to B \hspace{0.5cm} A }
        {B}
    &
    ( \to \mathcal{I} ) &
    \Inf { \infer*{B}{[A]} }
        { A \to B}
  \end{array}
  \]
  }
  
  The $\to\mathcal{E}$ rule says that given the statement $A \to B$ and the statement $A$,
  we can conclude $B$.
  For instance, consider 
 
  \[
  \begin{array}{lcl}
    A & = & \textit{It is raining} \\
    B & = & \textit{It is wet outside} \\
    A \to B & = & \textit{If (it is raining) then (it is wet outside)}
  \end{array}
  \]
  If we know that ``if (it is raining) then (it is wet outside)'' and we are told ``it is raining'',
  clearly we can take for granted ``it is wet outside''.
  
  The $\to\mathcal{I}$ rule says that if we assume $A$ and from that assumption we deduce $B$,
  we can conclude that $A \to B$.
  Let us assume (where $[A]$ denotes that we assume $A$ is true): 
  \[
    [A] = \textit{Turing did not see Church's work} 
  \]
  From this assumption, it's clear that that 
  \[
    B = \textit{Turing could not have stolen Church's work} 
  \]
  The $\to\mathcal{I}$ rule lets us conclude from these statements that
  \[
    A \to B = \textit{If Turing did not see Church's work, he could not have stolen it}
  \]

  By restricting ourselves to these rules, we are working within \emph{Implicative Intuitionistic Logic} (IIL).
  \footnote{Importantly, IIL also rejects the law of excluded middle which says for every $P$, $P \lor \neg P$.
  According to intuitionistic logic, unless we have a \emph{constructive} proof for which of $P$ or $\neg P$ is true, the statement is false}
  
  \subsection{Sequent Calculus}
  
  Gentzen explored natural deduction through the sequent calculus as well. 
  We have followed Girard \emph{et al.}'s observation that the syntax of the sequent calculus is overcomplicated for the purpose of natural deduction.\cite{Girard89}
  
  Sequent calculus manipulates \emph{sequents} where a sequent is denoted by:
  
    \[
      `G \vdash T
    \]
    
  On the left-hand side, the $`G$ represents a sequence of statements called the \emph{antecedent}.
  On the right-hand side of the $\vdash$, $T$ represents a different sequence of statements called the \emph{succedent}.
  The whole sequent denotes that the conjunction of the statements in the antecedent imply the disjunction of the statements in the succedent.
  That is to say, if all the statements on the left are true then at least one of the statements on the right is true:
  
  \[
    \begin{array}{c}
    A_1,A_2,\dots,A_n \vdash S_1,S_2,\dots,S_m \\
      \text{denotes} \\
    A_1 \wedge A_2 \wedge \dots \wedge A_n \to S_1 \lor S_2 \lor \dots \lor S_m \\
    \end{array}
  \]

  \subsection{Classical Logic}
  
  Gentzen's classical natural deduction used the sequent calculus and introduced a set structural rules for manipulating sequents:
  
  \definition{
  \textsc{Structural rules of the sequent calculus} 
  \[
  \begin{array}{c@{\quad\quad}c}
  
    \Inf{A,C,D,A' \vdash B}
       {A,D,C,A' \vdash B}
       {\hspace{5pt}\mathcal{L}X}
    &
    \Inf{A \vdash B,C,D,B'}
       {A \vdash B,D,C,B'}
       {\hspace{5pt}\mathcal{R}X}
    \\
    \\
    \Inf{A \vdash B}
       {A,C \vdash B}
       {\hspace{5pt}\mathcal{L}W}
    &
    \Inf{A \vdash B}
       {A \vdash B,C}
       {\hspace{5pt}\mathcal{R}W}
   \\
   \\
   \Inf{A,C,C \vdash B}
       {A,C \vdash B}
       {\hspace{5pt}\mathcal{L}C}
    &
    \Inf{A \vdash C,C,B}
       {A \vdash C,B}
       {\hspace{5pt}\mathcal{R}C}
   \\
   \\
   \multicolumn{2}{c}{
    \Inf{A \vdash B \hspace{10pt} A',B \vdash C}
        {A,A' \vdash C}
        {\hspace{5pt}\textbf{Cut}}
   }
  \end{array} 
  \]
  }
  
  The first set of rules are left and right exchange rules. 
  They express the commutativity of statements in the left and right sequences.
  The second set of rules, weakening rules, allows the introduction of new formula either the left or right sequences.
  The third set of rules, contraction rules, allow the contraction of multiple occurrences of a formula into a single occurrence. 
  The cut rule allows us to replace assumptions of formulae with their concrete proofs, if we have proved them somewhere else.
  
  In addition to these, the system maintains the standard introduction/elimination rules from natural deduction.
  The introduction/elimination rules have variants for manipulating the right sequence or the left sequence of statements.
  
  \subsection{Type Assignment}
  
  Type assignment introduces additional grammar and restrictions on
  the reduction rules of a system. These extensions prevent logically
  inconsistent terms from being constructed. A type assignment has the
  form:
  \[
    M : `a 
  \]
  which states that term $M$ has the type $`a$. Like variables, type 
  variables are abstract: they do not describe anything more about a 
  type than its identity. That is to say $x: A$ and $y : A$ have the
  same type but we cannot say any more about what that type is.
 
  A type is either some uppercase Latin letter or it is two valid types
  connected by a $\rightarrow$. This is described by the following
  BNF grammar:
  
  \definition{
    \textsc{(Grammar for constructing types)} 
    \item Type variables are represented by the lower-case greek alphabet $`a,`b,`g,...$
    \[
      A ::= `v \mid `v \rightarrow B 
    \]
  }
  
  \subsection{Typed \lam-Calculus}
  The typed \lam-calculus is an extension of the \lam-calculus with types assigned to \lam-terms.
  \lam-abstractions have arrow types: $A \to B$.
  This describes that the abstraction can be applied to a value of type $A$ and returns one of type $B$.
  Type assignment rules for the \emph{typed} \lam-calculus are:

  \[
    \begin{array}{c}
    \Inf{
      `G \vdash M : A \to B \hspace{20pt} N:A 
    }{
      `G \vdash M N : B 
    }
    \\
    \\
    \Inf{
      `G,x:A \vdash M : B
    }{
      `G \vdash `lx.M : A \to B 
    }
    \end{array}
  \]

  The first rule states that the application of a term of type $A \to B$ to
  a term of type $A$ has type $B$. 
  The second rule says a term of type $B$ in a context where $x:A$ is the same as an abstraction of type $A \to B$ in a context without $x:A$. 
  These rules add restrictions on what constitutes a well-formed term.
  These restrictions prevent the formation of terms sometimes undesirable properties.  
  
  \begin{example}[Type-assignment restricts set of valid terms]
  \[  
  \begin{array}{lr}
    (`lx.xx)(`lx.xx) \\
    xx : B \\
    x : A \to B \\
    x : A
  \end{array}
  \]
  The variable $x$ is applied to a term so it must have type $A \to B$.
  The term it is applied to must have type $A$.
  However $x$ is applied to itself so it must have type $A$ and $A \to B$.
  This means the term $`lx.xx$ is untypable:
  it is disallowed by the rules of the typed \lam-calculus.
  \end{example}
 
  \subsection{Curry-Howard Isomophism}
  
  The Curry-Howard isomorphism states that their is a true isomorphism between the type of a term and a logical proposition. 
  The type of a term is a logical proposition and the term itself its a proof of that proposition.
  The simplification of a proof maps to the evaluation of the corresponding program.\cite{Wadler15}
  
  Looking again at the rules of the typed \lam-calculus and IIL,
  the correspondence is clear:
  
  \[
  \begin{array}{cc@{\quad\quad}c}
    & `l\textbf{-calculus} & \textbf{IIL}
    \\
    \\
    \to\mathcal{E} 
    &
    \Inf{ `G \vdash M : A \to B \hspace{20pt} N:A }
        { `G \vdash M N : B }
    &
    \Inf{ A \to B \hspace{0.5cm} A }
        {B}
    \\
    \\
    \to\mathcal{I} 
    &
    \Inf{ `G,x:A \vdash M : B }
      { `G \vdash `lx.M : A \to B }
    &
    \Inf{ \infer*{B}{[A]} }
        { A \to B}
   \\
    \end{array}
  \]
 
  The correspondence between logic and programs is not limited to IIL and the typed lambda-calculus.
  There are many features of computer programs that have counter-parts in logical systems.
  % TODO: add more - examples and demonstration

\section{Haskell}
  % TODO: describe purity and laziness
  % TODO: add section on exceptions
  
  \subsection{Data Types}
  New data types can be introduced into Haskell in 3 distinct ways. First,
  using the \mono{data} keyword:
  
  \Verbatimcode
    data Animal a = Dog a
      | Cat
  \end{Verbatim}
  
  The \mono{data} keyword begins the definition of a new data type. The
  word immediately following determines the type constructor for the new
  type. Following this is a type parameter for the type constructor. There
  can be any number of type parameters, including zero. The right-hand
  side of the \mono{=} introduces a \mono{|}-separated list of data 
  constructors.
  
  \Verbatimcode
    > let hector = Cat
    > :t hector
    hector :: Animal a
    
    > let topaz = Dog "foo"
    > :t topaz
    topaz :: Animal String
  \end{Verbatim}
  
  The type parameter is constrained by the type of the value the data
  constructor was initialized with. In the example above, calling the
  \mono{Dog} data constructor with a string makes the type \mono{Animal
  String} rather than the more general \mono{Animal a}.
  
  The second method for introducing new data types is the \mono{newtype}
  keyword. The key difference between \mono{data} and \mono{newtype} is
  that \mono{newtype} can only have one data constructor. Informally,
  this implies a kind of isomorphism:

  \Verbatimcode
    newtype Foo a = Foo (a -> Integer)
  \end{Verbatim}
  
  The type constructor can take type parameters which will be constrained
  by the inhabitants of the data constructor. This data type expresses
  an isomorphism between \mono{Foo a} and functions from \mono{a} to
  \mono{Integer}s.
  
  Finally, we can introduce type aliases using the \mono{type} keyword:
  \Verbatimcode
    type Name = String 
  \end{Verbatim}
  
  Again, we introduce a type constructor \mono{Name} but this time we
  name another type, in this case \mono{String}, as its inhabitant. This
  means that the type \mono{Name} is a type alias for \mono{String} and
  will share the same data constructors. 
 
  \subsection{Type Level/Value Level}
  
  Haskell distinguishes between terms on the type level and terms on the value level. 
  This is the same as the separate layer of terms and types in the typed \lam-calculus.
  Types in Haskell are descriptions of the types of a value.
  They provide restrictions on the construction of invalid terms. 
  For instance if we have a function of type \mono{String -> Integer}, 
  we cannot apply it to a term of type \mono{Boolean}. 
  The type-checker will throw an error before any value-level computation is initiated.
 
  % TODO: better description needed
  The value level is the level on which data is constructed and manipulated.
  The operation \mono{1+1} occurs on the value level. The value level is
  where computation takes place and the type level is where static analysis
  of the program type takes place.
  
  \subsection{Type Classes}
  Haskell adds type classes to the type level. Types can have instances of
  type classes. The most similar concept from Object-Oriented programming
  is \emph{interfaces}.
  
  \Verbatimcode
    class Addable a where
      (add) :: a -> a -> a
  \end{Verbatim}
  
  Type classes are introduced using the \mono{class} keyword. Beneath that
  are the function names and corresponding type-signatures of the functions
  that an instance of a class must implement.
  
  For example, we can create instances of the \mono{Addable} class:
  
  \Verbatimcode
    data Number = One | Two | ThreeOrMore
    
    instance Addable Number where
      add One One = Two 
      add One Two = ThreeOrMore 
      add Two One = ThreeOrMore 
  \end{Verbatim}    
  
  To declare an instance of a type class, we must supply the bodies for functions in the class specification.
  The \mono{Addable} class, for instance, requires that the body of the \mono{add} function is defined.
  When declaring an instance, we have to ensure the type specification of each function is respected.
  
% TODO: these explanations of continuations (diff between delim and 
%       undelim) are incorrect 
\section{Continuations}
 
  As in the example in Figure \ref{fig:decomposing}, 
  compound \lam-terms can be decomposed into a dominant term and a context:
  
  \begin{figure}[!h]
    \hspace{1cm}Assume that $M \rightarrow_{`b} M^\prime$
    \[
    \begin{array}{lrcl}
    \textit{(Compound term)}&& MN \\
    \textit{(Decompose)}&M && \square N \\
    \textit{(Beta-reduce dominant term)}& M^\prime && \square N \\
    \textit{(Refill hole of context)}&& M^\prime N \\
    \end{array}
    \]
  \caption{Decomposing a term into a dominant term and a context}
  \end{figure}
  
  The reduction strategy will define how the term will be decomposed.
  When $M$ has \bta-reduced to a value, $M^\prime$, then the hole of the context $\square N$ is filled to form $M^\prime N$. 
  When the hole of a context is filled, the reduction of the compound term continues.
  This means the context contains the remaining terms to be reduced: it represents how reduction will continue. 
  Thus a context is also called a \emph{continuation}.
 
  \subsection{Undelimited Continuations} 
 
  For more complex terms, the waiting context will grow as the dominant
  term gets further decomposed:
  
  \begin{figure}[!h]
    \[
    \begin{array}{ll}
      (MM^\prime) M^{\prime\prime} \\
      (MM^\prime) & \square M^{\prime\prime} \\
      M & (\square M^\prime) M^{\prime\prime} \\
    \end{array}
    \]
  \caption{Decomposing a term into multiple contexts}
  \end{figure}

  By amalgamating continuations into one big continuation we only have
  two components at point during the reduction: the current dominant term
  and the \emph{current continuation}. 
 
  By adding additional operators to a language, the continuations of terms can be exposed.
  This provides programmers with the ability to control continuations.
  Control operators that only allow manipulation of the entire remaining continuation are \textbf{undelimited continuations}. 
  For example, Scheme's \mono{call/cc} operator aborts the entire remaining continuation.

  \subsection{Delimited-Continuations}

  Instead, if we maintain a stack of continuations when
  decomposing complex terms, we can keep continuations separated:
  
  \begin{figure}[!h]
    \[
    \begin{array}{lll}
      (MM^\prime) M^{\prime\prime} \\
      (MM^\prime) & \square M^{\prime\prime} \\
      M & \square M^\prime & \square M^{\prime\prime} \\
    \end{array}
    \]
  \caption{Decomposing a term into multiple contexts}
  \end{figure}

  Here, when a dominant term has been reduced, 
  the reduct is returned to the continuation at the top of the stack. 
  This newly joined term then becomes the dominant term. 
  After this new dominant term has been reduced, 
  it will be returned to the next waiting continuation, and so on. 
  Throughout this process, we maintain each continuation separately.

  If the control operators of a language allow manipulation of portions of the continuation stack,
  the continuations are \textbf{delimited}.
  To manipulate portions of the stack, these operators need a control delimiter.
  These are commonly called \emph{prompts}, after Felleisen first introduced them as such.
  By allowing prompts to be pushed onto the stack, we can recall portions of the stack up until a prompt.
  This gives the operators a finer grain of control.

  \subsection{Continuation-Passing Style}
 
  By rewriting \lam-terms, a term's continuation can be made explicit. All
  terms must be turned into \lam-abstractions of some variable $k$
  where $k$ is the continuation of a term. $k$ is then called on the
  result of the term, triggering the continuation to take control.
  This style of writing \lam-terms is called continuation-passing
  style or CPS.
  
  \definition{
    \textsc{(Translation of standard \lam-terms into CPS)} 
    \[
    \begin{array}{lcl}
      \tr{x}     & = & `lk.kx      \\
      \tr{`lx.M} & = & `lk.k(`lx.\tr{M}) \\
      \tr{M N} & = & `lk.M(`lm.m\tr{N}(`ln.mnk)
    \end{array}
    \]
  }
  
  The term that a CPS program terminates on will be of the form
  $`lk.kM$. In order to extract the value, a \emph{final continuation}
  must be provided. Depending on the context, this could be an identity 
  function $`lx.x$ or a display operation $`lx.\textsc{display }x$ to
  display the results of the program.

  \begin{figure}[!h]
  \caption{Extracting the final value from a terminated CPS program}
  \[
  \begin{array}{ll}
                & (`lk.kM)(`lx.x) \\
    \rightarrow & (`lx.x)M \\
    \rightarrow & M
  \end{array}
  \]
  \end{figure}
 
  The translation of standard \lam-terms into CPS similarly transforms the 
  \emph{types} of \lam-terms. For example, a term $x : A$ becomes 
  $`lk.kx : (A \to B) \to B$. This type represents a delayed computation:
  a computation that is waiting for a function to continue execution with. 
  In order to resume the computation, the term must be applied to a 
  continuation.
  
  As an example, take the term M where
  
  \[
    M = `lk.kx
  \]

  To access the value $x$ contained in $M$, we have to apply $M$ to a
  continuation function $`lm.N$:
  
  \[
  \begin{array}{rl}
      (`lk.kx)(`lm.N) \\
      \to & (`lm.N)x \\
      \to & N[x/m]
  \end{array}
  \]
  
  Within the body of $N$, $m$ is bound to the value contained by $M$.
  So we can think about $M$ as a suspended computation that, when applied
  to a continuation, applies the continuation to $x$. Looking at
  the type $(A \to B) \to B$ again, it is clear that $A$ is the type
  of the term passed to the continuation of a CPS-term:
  
  \[
  \begin{array}{l}
    `lk.kx : (A \to B) \to B \\
    k : (A \to B) \\
    kx : B \\
    x : A \\
  \end{array} 
  \]
  
  % TODO: explain how order-of-evaluation can be enforced using 
  %       cps.
 
  \subsection{Monads}
  
  If we have two suspended computations $M$ and $M^\prime$ and we want to
  run $M$ and then $M^\prime$, we have to apply $M$ to a continuation
  to access its value and then do the same to $M^\prime$:
  
  \[
    M(`lm.M^\prime(`lm^\prime.N))
  \]
  
  This is a common operation so we define a utility operator \mono{>>=}
  that binds the first suspended computation to a continuation which 
  returns another suspended computation\footnote{ \mono{(>>=)} is pronounced 'bind'.}:
 
  \[
    \mono{>>=} : ((A \to B) \to B) \to (A \to ((B \to C) \to C)) \to ((B \to C) \to C)
  \]
 
  The type $(A \to B) \to B$ that represents a suspended computation returning
  a value of type $A$ to its continuation we will call an $A$-computation or
  $Comp\ A$. We can rewrite the type signature of \mono{>>=}:
  
  \[
    \mono{>>=} : Comp\ A \to (A \to Comp\ B) \to Comp\ B
  \]
  
  We define another operator, $return$, that takes a value and returns a 
  suspended computation that returns that value:
  
  \[
    return : A \to Comp\ A 
  \]
 
  The type constructor $Comp\ A$, together with the two utility functions
  \mono{>>=} and $return$, make up Haskell's Monad type class:
  
  \Verbatimcode
    class Monad M where
      (>>=) :: M a -> (a -> M b) -> M b
      return :: a -> M a
  \end{Verbatim}
  
  The Monad type class generalizes CPS terms: they represent suspended
  computations that can be composed using \mono{>>=}. Just like CPS terms,
  a Monad type \mono{M a} tells us that we have a term that will pass
  values of type \mono{a} to the continuation it is bound to using
  \mono{>>=}.
  
  % TODO: finish explanation of linking suspended-computations and CPS
  %       to monads

\section{\lmu-Calculus}
  
  Michel Parigot wrote the \lmu-calculus as a system with an isomorphism to classical logic.
  It is an extension of the \lam-calculus. 
  This means that the grammar and reduction rules of the \lmu-calculus are a superset of those of the \lam-calculus.
  

  % TODO: intuitively, that mu-terms are expressed easily by continuation 
  %       passing style gives us some idea that they are about control flow
  %       and that they will be easily expressed by monads
  % TODO: add rules for consumption of multiple terms
  % TODO: add typing rules
  % TODO: add reduction strategy and information about normal form
  \subsection{Syntax}
  
  Just as \lam\ introduces \lam-abstractions, 
  $`m$ introduces $`m$-abstractions. 
  The body of a $`m$-abstraction must be a named term. 
  A named term consists of a name of the form $[`a]$ followed by an unnamed 
  term. 

  \begin{figure}[!h]
  \definition{ 
    \textsc{(Grammar for \lmu-calculus)}
    \item $`l$-variables are denoted by $x, y,\dots$ and $`m$-variables are denoted by $`a, `b,\dots$ \\
    \[
    \begin{array}{lrcl}
    
    \text{(Unnamed term)} & M,N & ::= & x\ |\ `lx.M\ |\ M\ N\ |\ `m`a.C \\
    \text{(Named term)} & C & ::= & [`a]M
    \end{array}
    \]
  }
  \end{figure}
  
  \subsection{Reduction Rules}
  \begin{figure}[!h]
  \definition{ 
    \textsc{(Reduction rules for \lmu-calculus)}
    \[
    \begin{array}{lrcll}
    i) & (`lx.M) N & \to_{`b} & M[N/x] \\
    ii) & (`m`a.[`b]M) N & \to{`m} & (`m`a.[`b]M[[`g]M^\prime N/[`a]M^\prime]) \\
    iii) & `m`a.[`a]M & \to{`m} & M & (`a \notin fn(M)) \\
    iv) & `m`d.[`b](`m`g.[`a]M) & \to{`m} & `m`d.[`a]M[`b/`g] \\
    \end{array}
    \]
  }
  \end{figure}

  The terse reduction rule of $iii)$ simply states that the application of a \lmu-abstraction $`m`a.M$ to a term $N$ applies all the sub-terms of $M$ labelled $[`a]$ to $N$ and relabels them with a fresh $`m$ variable.
  It can be thought of as a \lam-abstraction that can be applied to any number of variables \cite{Parigot92}.
  If we knew how many variables the term is applied to, we could replace
  \[ `m`a\dots[`a]M \]
  with 
  \[ `lx_1\dots`lx_n\dots Mx_1\dots x_n \]
  
  \subsection{Computational Significance}

  The \lmu-calculus makes use of \emph{applicative contexts}:
  An \lmu-abstraction applied to some term $N$ points the $`m$-variable to the context $\square N$.
  In this sense, the \lmu-abstraction captures contexts where the contexts are supplied by application.
  When an unnamed term is labelled with a \lmu-variable, it is evaluated in that context. 
  For instance the named term $[`a]M$ has the effect of evaluating $M$ in the context pointed to by $`a$.
  
  By translating $`m$-terms into CPS, we can see the mapping between $`m$-variables and contexts more clearly:
  \definition{
  \textsc{CPS translation of $`m$-terms} \cite{Groote94}
  \[
  \begin{array}{rcl}   
    \tr{`m`a.M} & \triangleq & `l`a.\tr{M} \\
    \tr{[`a]M} & \triangleq & `lk.\tr{M}\ `a\ k
  \end{array}
  \]
  }
  A $`m$-abstraction binds a continuation to a variable for use throughout the abstraction's body.
  A named term applies the term to the name, effectively running the term in the continuation.
  
  To make this more concrete, consider the compound term $(`m`a.[`b]M)\ N$. 
  First we decompose the term into a dominant term $(`m`a.[`b]M)$ and a 
  context $\square N$. Informally, we can imagine that the variable at the head of the $`m$-abstraction now maps to this context $\{`a \Rightarrow \square N\}$:
  
  % TODO: reform to get rid of context mapping:
  %   keep mu abstraction in dominant and consult context for
  %   what to apply named-terms to
  \begin{example}[]
    \[
    \begin{array}{lc}
    \textbf{Dominant} & \textbf{Context} \\
    (`m`a.[`b]M) N \\
    `m`a.[`b]M & \square N \\
    \end{array}
    \]
  \end{example}

  All subterms of $M$ labelled $`a$ will now be evaluated in the context 
  $\square N$ and the context will destroyed. For example, let us replace 
  $M$ with \mbox{$`m\nonocc.[`a](`ls.fs)$}:
  \footnote{Following van Bakel, we use $\nonocc$ to denote a $`m$-variable that does not occur in the body of the \lmu-abstraction.}
  
  \begin{example}
    \[
    \begin{array}{lcr}
    \textbf{Dominant} & \textbf{Context} \\
    `m`a.[`b]`m\circ.[`a](`ls.fs)    & \square N \\
    `m`a.[`a](`ls.fs)    & \square N \\
    `m`g.[`g](`ls.fs)N   & & (`g\ \text{fresh})  \\
    \end{array}
    \]
  \end{example}

  % TODO: explain applicative contexts
  After applying the term $[`a](`ls.fs)$ to $N$, 
  the context $\square N$ is consumed and every occurrence of $`a$ is replaced with a fresh variable 
  -- in this case a $`g$ -- 
  to clarify that the $`m$-abstraction now points to a new context. 
  This means that $`m$-abstractions will pass all of the applicative contexts to the named subterms:
  
  \begin{example}
    \[
    \begin{array}{lcr}
    \textbf{Dominant} & \textbf{Context} \\
    (`m`a.[`a](`ls.`lt.st)) M N \\
    (`m`a.[`a](`ls.`lt.st))M & \square N \\
    `m`a.[`a](`ls.`lt.st) & \square M:\square N \\
    `m`g.[`g](`ls.`lt.st)M & \square N & (`g\ \text{fresh}) \\
    `m`d.[`d](`ls.`lt.st)MN & \square N & (`d\ \text{fresh}) \\
    \end{array}
    \]
  \end{example}
  
  \subsection{Curry-Howard Isomorphism}
  The typed variant of the \lmu-calculus is isomorphic to classical natural deduction.
  The type assignment rules for the typed \lmu-calculus are as follows:
  
  \definition{
  \textsc{Typing rules for the typed \lmu-calculus}
  \[
  \begin{array}{c@{\quad\quad}c}
    \Inf{`G \vdash M : B\ |\  `a : A, `b : B, `D}
        {`G \vdash `m`a.[`b]M : A |\ `b : B, `D}
    &
    \Inf{`G \vdash M : A\ |\ `a : A, `D}
        {`G \vdash `m`a.[`a]M : A\ |\ `D}
  \end{array}
  \]
  }
  
  These rules correspond to the structural rules of classical natural deduction.
  The original presentation of Parigot's type assignment makes the isomorphism clearer.
  Our presentation here makes it easier to understand the role of the operators.

\section{Calculus of Delimited-Continuations}

  % TODO: explain motivation behind DCC: \lmu requires
  %       delimited continuations and DCC formalizes delimited
  %       continuations in haskell so useful guide for implementing
  %       exceptions in haskell

  Simon Peyton-Jones \textit{et al.}\ extended the \lam-calculus with additional operators in order create a framework for implementing delimited continuations \cite{JonesDS07}. 
  This calculus will be referred to as the calculus of delimited-continuations or \emph{CDC}. 
  Many calculi have been devised with control mechanisms for manipulating continuations, like the \lmu-calculus.
  These control mechanisms manipulate either delimited and undelimited continuations. 
  CDC provides a set of operations that are capable of expressing many of the common control mechanisms found in the literature.

  \subsection{Syntax}
  The grammar of CDC is an extension of the standard \lam-calculus:

  \begin{figure}[!h]
  \definition{ 
    \textsc{(Grammar for CDC)}
    \[
    \begin{array}{lrcl}
    \textrm{(Variables)} & x, y, \dots \\
    \textrm{(Expressions)} & e & ::= & x\ |\ `lx.e\ |\ e\ e^\prime \\
                           &   &  |  &  newPrompt\ |\ pushPrompt\ e\ e \\
                           &   &  |  &  withSubCont\ e\ e\ |\ pushSubCont\ e\ e
    \end{array}
    \]
  }
  \end{figure}

  \subsection{Reduction Rules}
  
  The operational semantics can be understood through an abstract machine that transforms tuple of the form $\langle e,\ D,\ E,\ q \rangle$.
  The tuple consists of:
    \begin{itemize}
      \item $e$ - the current dominant term
      \item $D$ - the current context/continuation
      \item $E$ - the stack of remaining continuations
      \item $q$ - a global counter for producing fresh prompt values
    \end{itemize}
  By representing terms in this way, 
  the reduction rules are able to make the control of terms and their continuations more explicit.
  
  The abstract machine also introduces some Haskell-style notation for dealing with sequences. An empty sequence is represented by $[]$. 
  A value added to the head of a list is represented by $:$ for instance $D:[]$. $\app$ represents two lists appended together. 
  $E\until{p}$ and $E\from{p}$ denote the subsequence of $E$ \emph{until} prompt $p$ and \emph{from} prompt $p$, respectively. 
  Neither of these subsequences contain $p$.

  \begin{figure}[!h]\label{fig:cdc-abstract-machine}
  \relscale{0.9}
  \definition{ 
    \textsc{(Operational semantics for CDC)} \cite{JonesDS07}
    \[
    \begin{array}{lcll}
      \langle e\ e^\prime, D, E, q \rangle &\to &\langle e, D[\square\ e^\prime], E, q \rangle &\text{e non-value} \\
      \langle v\ e, D, E, q \rangle &\to &\langle e, D[v\ \square], E, q \rangle &\text{e non-value} \\
      \langle pushPrompt\ e\ e^\prime, D, E, q \rangle &\to &\langle e, D[pushPrompt\ \square\ e^\prime], E, q \rangle &\text{e non-value} \\
      \langle withSubCont\ e\ e^\prime, D, E, q \rangle &\to &\langle e, D[withSubCont\ \square\ e^\prime], E, q \rangle &\text{e non-value} \\
      \langle withSubCont\ p\ e, D, E, q \rangle &\to &\langle e, D[withSubCont\ p\ \square], E, q \rangle &\text{e non-value} \\
      \langle pushSubCont\ e\ e^\prime, D, E, q \rangle &\to &\langle e, D[pushSubCont\ \square\ e^\prime], E, q \rangle &\text{e non-value} \\
    \\
      \langle (`lx.e)\ v, D, E, q \rangle &\to &\langle e[v/x], D, E, q \rangle \\
      \langle newPrompt, D, E, q \rangle &\to &\langle q, D, E, q+1 \rangle \\
      \langle pushPrompt\ p\ e, D, E, q \rangle &\to &\langle e, \square, p : D : E, q \rangle \\
      \langle withSubCont \ p\ v, D, E, q \rangle &\to &\langle v (D : E\until{p}, \square, E\from{p}, q \rangle \\
      \langle pushSubCont E^\prime\ e, D, E, q \rangle &\to &\langle e, \square, E^\prime \app (D : E), q \rangle \\
    \\
      \langle v, D, E, q \rangle &\to &\langle D[v], \square, E, q \rangle \\
      \langle v, \square, p : E, q \rangle &\to &\langle v, \square, E, q \rangle \\
      \langle v, \square, D : E, q \rangle &\to &\langle v, D, E, q \rangle
    \end{array}
    \]
  }
  \end{figure}
  
  \subsection{Significance}

  % TODO: ensure prompts and continuation stack has been explained before reaching this point
  The additional terms behave as follows:
  \begin{itemize}
  \item \op{newPrompt} returns a new and distinct prompt.
  \item \op{pushPrompt}'s first argument is a prompt which is pushed onto the continuation stack before evaluating its second argument. 
  \item \op{withSubCont} captures the subcontinuation from the most recent occurrence of the first argument (a prompt) on the excution stack to the current point of execution. Aborts this continuation and applies the second argument (a \lam-abstraction) to the captured continuation.
  \item \op{pushSubCont} pushes the current continuation and then its first argument (a subcontinuation) onto the continuation stack before evaluating its second argument.
  \end{itemize}
  
  The abstract machine defined in Figure \ref{fig:cdc-abstract-machine} also encodes the reduction strategy.
  The first block of rules define in what order redexes are reducted.
 
\section{\ltry-Calculus}

Steffen van Bakel extended the \lam-calculus with operators for modelling exceptions.
Unlike previous systems, the \ltry-calculus uses named exceptions.

\subsection{Exceptions}

Exceptions in programming languages are indications that control flow cannot continue.
For example, if you attempt to open a non-existent file,
the operation might throw an exception.
If an exception occurs without being caught, a program will exit with an error.
Exceptions can be caught and attempts at recovery can be made by exception handlers.

The common syntax for introducing exception handlers is in try-catch blocks.
An exception that occurs in a try-catch block will be handled by a corresponding handler.
For example, see the Javascript syntax for this:

\begin{Verbatim}
  try {
    /* possibly throw exception */
  } catch (e) {
    /* recover thrown exception */ 
  }
\end{Verbatim}

The \mono{catch (e) \{ ... \}} introduces a single exception handler.
This exception handler will be called if an exception is thrown inside the try block.
If we want to introduce multiple exception handlers,
we need a mechanism for deciding which handler will be called.
Java solves this by registering different exception handlers based on their type:

\begin{Verbatim}
  try {
    /* possibly throw exception */
  } catch (IOException e) {
    /* recover from IOException */  
  } catch (FileNotFoundException e) {
    /* recover from FileNotFoundException */ 
  }
\end{Verbatim}

Here, which handler is called depends on the type of the exception thrown.

\subsection{Syntax}

The grammar of the \ltry-calculus is as follows:
\definition{
\textsc{(Grammar of the \ltry-calculus)}
\[
  \begin{array}{rclr}
    C &::=& \catch{ n$_1$($x$) = $M_1$ }; \dots; \catch{n$_i$($x$) = $M_i$} & (i \geq 1) \\
    M,N &::=& x\ |\ `lx.M\ |\ MN\ |\ \try M;\ C\ |\ \throw{n($M$)}
  \end{array}
\]
}

The grammar for $C$ describes a catch block as series of one or more catch statements. 
For convenience, we will use the notation 
\[
  \mult{\catch{n$_i$($x$) = $M_i$}}
\]
to describe a catch block with more than one catch statement. 

The \ltry-calculus adds three new syntactic constructs:
\begin{itemize}
\item \textbf{throw n($M$)} denotes the throwing of an exception with name $n$ passing it the value $M$.
\item \textbf{catch n($x$) = $M$} registers the exception handler $M$ to the name $n$ with parameter $x$.
\item \textbf{try $M$; $C$} attempts to run term $M$ in an environment with the exception handlers in catch block $C$ registered.
\end{itemize}

\subsection{Reduction Rules}
In conjunction with the additional syntactic constructions,
the \ltry-calculus introduces some reduction rules:

\definition{
\textsc{(\ltry\ reduction rules)}
\[
\begin{array}{rlcl}
  \text{($`b$):}      & (`lx.M)N &\to& M[N/x] \\
  \text{(throw):}     & (\throw{n($M$)})N &\to& \throw{n($M$)} \\
  \text{(try-throw):} & \try \throw{n$_l$($N$)};\ \mcatch &\to& M_l[N/x] \\
  \text{(try-value):} & \try V;\ \mcatch &\to& V \\
\end{array}
\]
}

The $`b$ reduction rule is familiar from the \lam-calculus. 
A \textbf{throw} term applied to any term discards the second term.
A \textbf{try} term that contains a throw reduces to the handler that corresponds to the name of the exception thrown with all occurrences of the parameter replaced by the value thrown. 
For instance a $\throw{n(N)}$ inside a \textbf{try} will reduce to $M[N/x]$ if there is a $\catch{n($x$) = $M$}$ in the catch block.
A \textbf{try} term that contains a value reduces to just that value.

\subsection{Significance}
The occurrence of an exception aborts the current computation.
\ltry\ models this by discarding terms that a \textbf{throw} is applied to.
The \textbf{try-catch} statements mirror the syntax of try-catch statements in programming languages in the C-syntax family.

